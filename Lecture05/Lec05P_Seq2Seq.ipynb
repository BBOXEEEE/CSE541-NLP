{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6DrXb_sSNi2"
      },
      "outputs": [],
      "source": [
        "# !pip install lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZtFhw78nR9DQ"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bi5ktgenR9DR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZIP file downloaded to fra-eng.zip\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def download_zip(url, output_path):\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"ZIP file downloaded to {output_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
        "\n",
        "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "output_path = \"fra-eng.zip\"\n",
        "download_zip(url, output_path)\n",
        "\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, output_path)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_m0RYmdeR9DS"
      },
      "outputs": [],
      "source": [
        "def to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-4mvFOTxR9DS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ],
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jj5i4llPR9DS"
      },
      "outputs": [],
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input = [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"[SOS] \" + tar_line + \" [EOS]\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "\n",
        "  return encoder_input, decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tqU8ow9nR9DS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인코더의 입력 : [['i', 'went', 'drinking', 'with', 'one', 'of', 'my', 'boyfriend', 's', 'friends', 'and', 'now', 'he', 's', 'furious', 'at', 'me', '.', 'was', 'this', 'friend', 'a', 'guy', 'or', 'a', 'girl', '?', 'a', 'guy', 'obviously', '.', 'why', 'would', 'i', 'go', 'drinking', 'with', 'his', 'female', 'friends', '?', 'yeah', 'you', 're', 'right', '.', 'his', 'name', 'is', 'tom', '.', 'he', 's', 'really', 'hot', 'and', 'i', 'really', 'want', 'to', 'go', 'drinking', 'with', 'him', 'again', '.']]\n",
            "디코더의 입력 : [['[SOS]', 'je', 'suis', 'allee', 'boire', 'avec', 'un', 'ami', 'de', 'mon', 'compagnon', 'et', 'voila', 'qu', 'il', 'est', 'furieux', 'contre', 'moi', '.', 'etait', 'ce', 'un', 'gars', 'ou', 'une', 'fille', '?', 'un', 'gars', 'bien', 'evidemment', '.', 'pourquoi', 'irais', 'je', 'boire', 'avec', 'ses', 'amies', '?', 'ouais', 'ca', 'se', 'comprend', '.', 'il', 's', 'appelle', 'tom', '.', 'il', 'est', 'trop', 'canon', 'et', 'j', 'ai', 'tellement', 'envie', 'd', 'aller', 'prendre', 'un', 'verre', 'avec', 'lui', 'a', 'nouveau', '.', '[EOS]']]\n"
          ]
        }
      ],
      "source": [
        "sents_en_in, sents_fra_in = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[-1:])\n",
        "print('디코더의 입력 :',sents_fra_in[-1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(232736, 232736)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sents_en_in), len(sents_fra_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iRB7IUpQR9DT"
      },
      "outputs": [],
      "source": [
        "#split data -> train-validation-test로 구분\n",
        "def split_data(data, train_ratio=0.7, shuffle=True):\n",
        "    data = list(data)\n",
        "    if shuffle:\n",
        "        random.shuffle(data)\n",
        "    n_train = int(len(data) * train_ratio)\n",
        "    train_data = data[:n_train]\n",
        "    test_data = data[n_train:]\n",
        "    return train_data, test_data\n",
        "\n",
        "# 데이터에서 적당한 크기로 분할\n",
        "# 데이터가 적으면 보통 8:2\n",
        "# 데이터가 많으면 보통 9:1\n",
        "# validation data는 항상 train data에서!\n",
        "train_test_ratio = 0.9\n",
        "train, test = split_data(zip(sents_en_in, sents_fra_in), train_test_ratio)\n",
        "train, vali = split_data(train, train_test_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j5kQ8st8R9DT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(188515, 20947, 23274)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train), len(vali), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TIEdya8AR9DT"
      },
      "outputs": [],
      "source": [
        "# make vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "# 영어(인코더 입력)에 대한 vocab, 프랑스어(디코더 입력)에 대한 vocab\n",
        "en_token_cnt = Counter()\n",
        "fr_token_cnt = Counter()\n",
        "\n",
        "for tokens, _ in train:\n",
        "    en_token_cnt.update(tokens)\n",
        "\n",
        "min_count = 2\n",
        "en_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
        "for token, count in en_token_cnt.items():\n",
        "    if count > min_count and token not in en_vocab:\n",
        "        en_vocab[token] = len(en_vocab)\n",
        "\n",
        "\n",
        "for _, tokens in train:\n",
        "    fr_token_cnt.update(tokens)\n",
        "\n",
        "fr_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
        "for token, count in fr_token_cnt.items():\n",
        "    if count > min_count and token not in fr_vocab:\n",
        "        fr_vocab[token] = len(fr_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UnzBQRsYR9DT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8320, 11847)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en_vocab), len(fr_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qAPc7cbZR9DT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qmJalyn7R9DT"
      },
      "outputs": [],
      "source": [
        "class EnToFrDataset(Dataset):\n",
        "    def __init__(self, data, en_vocab, fr_vocab):\n",
        "        self.enc_input = []     # 인코더의 입력 영어 데이터\n",
        "        self.dec_input = []     # 디코더의 입력 프랑스어 데이터\n",
        "        self.dec_target = []\n",
        "        for en_sent, fr_sent_in in data:\n",
        "            self.enc_input.append(en_sent)\n",
        "            self.dec_input.append(fr_sent_in)\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.max_len = 30       # 데이터가 가질 수 있는 최대 길이\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.enc_input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 인코더와 디코더에 들어갈 샘플\n",
        "        # source, target에 대한 index sequence\n",
        "        src_sample = [self.en_vocab.get(w, self.en_vocab.get(\"[UNK]\")) for w in self.enc_input[idx]]\n",
        "        # print(src_sample)\n",
        "        trg_sample = [self.fr_vocab.get(w, self.fr_vocab.get(\"[UNK]\")) for w in self.dec_input[idx]]\n",
        "        # print(trg_sample)\n",
        "        # truncate and padding\n",
        "        src_sample = src_sample[:self.max_len]\n",
        "        trg_sample = trg_sample[:self.max_len]\n",
        "        src_sample += [self.en_vocab.get(\"[PAD]\")] * (self.max_len - len(src_sample))\n",
        "        trg_sample += [self.fr_vocab.get(\"[PAD]\")] * (self.max_len - len(trg_sample))\n",
        "\n",
        "        # dictonary 형태로, 같은 key를 가진 것끼리 batch가 만들어진다.\n",
        "        return {\"src\": torch.LongTensor(src_sample), \"trg\": torch.LongTensor(trg_sample)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qECJV3gBR9DU"
      },
      "outputs": [],
      "source": [
        "train_dataset = EnToFrDataset(train, en_vocab, fr_vocab)\n",
        "vali_dataset = EnToFrDataset(vali, en_vocab, fr_vocab)\n",
        "test_dataset = EnToFrDataset(test, en_vocab, fr_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64,drop_last=True, shuffle=True, num_workers=8)\n",
        "vali_loader = DataLoader(vali_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BDBXcONrR9DU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 109, 53, 56, 575, 228, 4, 104, 524, 10]\n",
            "[2, 4, 16, 679, 351, 118, 201, 239, 4, 20, 586, 10, 3]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'src': tensor([  4, 109,  53,  56, 575, 228,   4, 104, 524,  10,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0]),\n",
              " 'trg': tensor([  2,   4,  16, 679, 351, 118, 201, 239,   4,  20, 586,  10,   3,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0])}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__getitem__(10120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'src': tensor([[  63,  212,   48, 2058,  180,   15,  517,   57,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0],\n",
            "        [  84,   39,  149,   42, 1671, 1783,  461,   22,  281,   10,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0]]), 'trg': tensor([[   2,   44,  598,  220,   29, 3294, 4503,   45,   63,    3,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0],\n",
            "        [   2,   95,   44,   44,  257, 1294,   16,  201,  894,   48,   24,   67,\n",
            "          299,   10,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0]])}\n"
          ]
        }
      ],
      "source": [
        "# DataLoader 동작 Test\n",
        "a = DataLoader(train_dataset, batch_size=2,drop_last=True, shuffle=True, num_workers=8)\n",
        "for batch in a:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z_O6kBtBR9DU"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    outputs, (hidden, cell) = self.rnn(embedded)\n",
        "    return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Aw9Tp2yiR9DU"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)    # output_dim, output_dim 같은데 굳이 embedding과 linear 둘다 사용, 개선 고민해볼것!\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # 인코더의 hidden, cell state를 디코더의 LSTM 입력으로\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m5p--x9DR9DU"
      },
      "outputs": [],
      "source": [
        "# decoder with simple dot product attention\n",
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # unsqueeze : 토큰이 하나씩 들어가기 때문에 차원을 맞추기 위한 연산\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # print(output.size())\n",
        "        # print(encoder_outputs.size())\n",
        "\n",
        "        attention_score = torch.bmm(output.squeeze(0).unsqueeze(1), encoder_outputs.permute(1, 2, 0)).squeeze(1)\n",
        "        attention_distribution = torch.softmax(attention_score, dim=1)\n",
        "        context = torch.bmm(attention_distribution.unsqueeze(1), encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(0), context), dim=1))\n",
        "        # print(attention_score.size())\n",
        "        # print(attention_distribution.size())\n",
        "        # print(context.size())\n",
        "        # print(prediction.size())\n",
        "\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lqOKsBduR9DU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "import lightning as pl\n",
        "\n",
        "class Seq2Seq(pl.LightningModule):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.criterian = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.automatic_optimization = False\n",
        "        # 인코더, 디코더 따로 있기 때문에 auto_optimizer를 사용할 수 없다.\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tar_len, batch_size, trg_vocab_size 만큼의 공간\n",
        "        # 디코더의 출력을 저장하는 공간\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(trg.device)\n",
        "\n",
        "        enc_output, hidden, cell = self.encoder(src)\n",
        "        # print(enc_output.size())\n",
        "        # print(hidden.size())\n",
        "        # print(cell.size())\n",
        "        \n",
        "        # 하나의 token씩 입력을 넣어준다.\n",
        "        input = trg[0,:]\n",
        "        # print(input)\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            if isinstance(self.decoder, AttentionDecoder):\n",
        "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
        "            else:\n",
        "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "                # print(output.size())\n",
        "                # print(hidden.size())\n",
        "                # print(cell.size())\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        enc_opt, dec_opt = self.optimizers()\n",
        "\n",
        "        enc_opt.zero_grad()\n",
        "        dec_opt.zero_grad()\n",
        "\n",
        "        src = batch[\"src\"].permute(1, 0)    # LSTM에는 batch가 두번째로!\n",
        "        trg = batch[\"trg\"].permute(1, 0)\n",
        "\n",
        "        outputs = self(src, trg)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        outputs = outputs[1:].view(-1, outputs_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.manual_backward(loss)\n",
        "        enc_opt.step()\n",
        "        dec_opt.step()\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_PPL\", math.exp(loss))   # PPL : Launguae Generation 할 때의 성능지표!\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src = batch[\"src\"].permute(1, 0)\n",
        "        trg = batch[\"trg\"].permute(1, 0)\n",
        "\n",
        "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        outputs = outputs[1:].view(-1, outputs_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_PPL\", math.exp(loss))\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        src = batch[\"src\"].permute(1, 0)\n",
        "        trg = batch[\"trg\"].permute(1, 0)\n",
        "\n",
        "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        outputs = outputs[1:].view(-1, outputs_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_PPL\", math.exp(loss))\n",
        "        return loss\n",
        "\n",
        "    # 디코더의 실제 출력을 확인\n",
        "    def decode(self, src):\n",
        "        enc_output, hidden, cell = self.encoder(src.unsqueeze(1))\n",
        "        trg_len = 30\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = [2]\n",
        "        input = torch.LongTensor([2]).to(src.device)\n",
        "        for t in range(1, trg_len):\n",
        "            if isinstance(self.decoder, AttentionDecoder):\n",
        "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
        "            else:\n",
        "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            top1 = output.argmax(1)\n",
        "            outputs.append(top1.item())\n",
        "            if top1.item() == 3:\n",
        "                break\n",
        "            input = top1\n",
        "        return outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        enc_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-4)\n",
        "        dec_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=1e-4)\n",
        "        return enc_optimizer, dec_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1U4mCOOaR9DV"
      },
      "outputs": [],
      "source": [
        "emb_dim = 256\n",
        "hid_dim = 512\n",
        "n_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CypoauN7R9DV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(input_dim=len(en_vocab),\n",
        "                  emb_dim=emb_dim,\n",
        "                  hid_dim=hid_dim,\n",
        "                  n_layers=n_layers,\n",
        "                  dropout=0.5)\n",
        "\n",
        "decoder = Decoder(output_dim=len(fr_vocab),\n",
        "                    emb_dim=emb_dim,\n",
        "                    hid_dim=hid_dim,\n",
        "                    n_layers=n_layers,\n",
        "                    dropout=0.5)\n",
        "\n",
        "att_decoder = AttentionDecoder(output_dim=len(fr_vocab),\n",
        "                            emb_dim=emb_dim,\n",
        "                            hid_dim=hid_dim,\n",
        "                            n_layers=n_layers,\n",
        "                            dropout=0.5)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "att_model = Seq2Seq(encoder, att_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([2, 64, 512])\n",
            "torch.Size([2, 64, 512])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    src = batch[\"src\"].permute(1,0)\n",
        "    trg = batch[\"trg\"].permute(1,0)\n",
        "    model.forward(src=src, trg=trg)\n",
        "    break\n",
        "\n",
        "# 인코더의 출력들 예시\n",
        "# sequence의 길이, batch_size, hidden_size\n",
        "# [30, 64, 512] : 인코더의 output_size\n",
        "# [2, 64, 512] : 인코더의 hidden_state_size\n",
        "# [2, 64, 512] : 인코더의 cell_state_size\n",
        "\n",
        "# [2, 2, ...] : token 하나씩 입력, 첫 시작 토큰은 [SOS]\n",
        "\n",
        "# 디코더의 입력\n",
        "# [64, 11843] : 디코더의 output_size, [batch, 프랑스어 토큰 사이즈]\n",
        "# [2, 64, 512] : 디코더의 hidden_state_size\n",
        "# [2, 64, 512] : 디코더의 cell_state_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n",
            "torch.Size([1, 64, 512])\n",
            "torch.Size([30, 64, 512])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 30])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 11847])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    src = batch[\"src\"].permute(1,0)\n",
        "    trg = batch[\"trg\"].permute(1,0)\n",
        "    att_model.forward(src=src, trg=trg)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "FE7Cfr43R9DV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_att\", group=\"Lec05\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "0QpYo02JR9DV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=1,\n",
        "    accelerator=\"gpu\",\n",
        "    logger=wandb_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8orzEPCpR9DV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory ./NLP/yhvt2ljv/checkpoints exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | encoder   | Encoder          | 5.8 M \n",
            "1 | decoder   | Decoder          | 12.8 M\n",
            "2 | criterian | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "18.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "18.6 M    Total params\n",
            "74.387    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [03:45<00:00, 13.06it/s, v_num=2ljv]   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [03:45<00:00, 13.04it/s, v_num=2ljv]\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, train_loader, vali_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | encoder   | Encoder          | 5.8 M \n",
            "1 | decoder   | AttentionDecoder | 18.9 M\n",
            "2 | criterian | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "24.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "24.7 M    Total params\n",
            "98.649    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [05:00<00:00,  9.81it/s, v_num=kicz]   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [05:00<00:00,  9.79it/s, v_num=kicz]\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(att_model, train_loader, vali_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 363/363 [00:09<00:00, 37.58it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test_PPL             95.06916809082031\n",
            "        test_loss            4.547243118286133\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 4.547243118286133, 'test_PPL': 95.06916809082031}]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ug7qOGPBR9DV"
      },
      "outputs": [],
      "source": [
        "test_data = test_dataset.__getitem__(1000)\n",
        "a = att_model.decode(test_data[\"src\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "2Jqbv_MTR9DV"
      },
      "outputs": [],
      "source": [
        "input = \" \".join([list(en_vocab.keys())[list(en_vocab.values()).index(i)] for i in test_data[\"src\"]])\n",
        "target = \" \".join([list(fr_vocab.keys())[list(fr_vocab.values()).index(i)] for i in test_data[\"trg\"]])\n",
        "model_output = \" \".join([list(fr_vocab.keys())[list(fr_vocab.values()).index(i)] for i in a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "9rrCZFnDR9DW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i m not that kind of guy . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[SOS] je ne suis pas ce genre de gars . [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[SOS] je ne suis pas de de . . [EOS]\n"
          ]
        }
      ],
      "source": [
        "print(input)\n",
        "print(target)\n",
        "print(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "9CeHHDODR9DW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▃▃▃█▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>50.06936</td></tr><tr><td>train_loss</td><td>3.91341</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>79.08675</td></tr><tr><td>val_loss</td><td>4.36085</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Seq2Seq_att</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/4vf3kicz/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/4vf3kicz/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240417_120212-4vf3kicz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
