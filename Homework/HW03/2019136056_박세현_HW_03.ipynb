{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어처리 과제 3 (6주차)\n",
    "* 과제는 해당 .ipynb 파일에 코드 작성\n",
    "    * 코드는 google colab의 gpu를 사용하는 런타임 환경에서 모두실행을 통해 한번에 실행 되어야함\n",
    "    * 생성형 AI (ChatGPT, Copilot, Claude, ...) 등 사용 가능\n",
    "        * 단, 사용시 사용한 방법, 입력, 출력을 캡처해 보고서에 기입\n",
    "* Word를 통해 자유형식으로 보고서를 작성\n",
    "    * 보고서의 양식은 자유\n",
    "    * 보고서의 제출은 .pdf 형식으로 제출해야하며, 파일명은 \"학번_이름_HW_??.pdf\"로 제출 할 것\n",
    "    * 보고서에 코드를 그대로 복붙 하지 말 것 (캡처 도구를 활용, 환경 설치 자료 참고)\n",
    "* .ipynb와 .pdf 파일을 el을 통해 제출\n",
    "    * 예시 : \"2232036006_임상훈_HW_01.ipynb\"와 \"2232036006_임상훈_HW_01.pdf\"를 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 관련 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to fra-eng.zip\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "output_path = \"fra-eng.zip\"\n",
    "download_zip(url, output_path)\n",
    "\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, output_path)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "  # 프랑스어 악센트(accent) 삭제\n",
    "  # 예시 : 'déjà diné' -> deja dine\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "  # 악센트 제거 함수 호출\n",
    "  sent = to_ascii(sent.lower())\n",
    "\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) \"I am a student.\" => \"I am a student .\"\n",
    "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input = [], []\n",
    "\n",
    "  with open(\"fra.txt\", \"r\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "      # source 데이터와 target 데이터 분리\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"[SOS] \" + tar_line + \" [EOS]\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "\n",
    "  return encoder_input, decoder_input\n",
    "\n",
    "sents_en_in, sents_fra_in = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data -> train-validation-test로 구분\n",
    "def split_data(data, train_ratio=0.7, shuffle=True):\n",
    "    data = list(data)\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    n_train = int(len(data) * train_ratio)\n",
    "    train_data = data[:n_train]\n",
    "    test_data = data[n_train:]\n",
    "    return train_data, test_data\n",
    "\n",
    "train_test_ratio = 0.9\n",
    "train, test = split_data(zip(sents_en_in, sents_fra_in), train_test_ratio)\n",
    "train, vali = split_data(train, train_test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "# 영어(인코더 입력)에 대한 vocab, 프랑스어(디코더 입력)에 대한 vocab\n",
    "en_token_cnt = Counter()\n",
    "fr_token_cnt = Counter()\n",
    "\n",
    "for tokens, _ in train:\n",
    "    en_token_cnt.update(tokens)\n",
    "\n",
    "min_count = 2\n",
    "en_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
    "for token, count in en_token_cnt.items():\n",
    "    if count > min_count and token not in en_vocab:\n",
    "        en_vocab[token] = len(en_vocab)\n",
    "\n",
    "\n",
    "for _, tokens in train:\n",
    "    fr_token_cnt.update(tokens)\n",
    "\n",
    "fr_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
    "for token, count in fr_token_cnt.items():\n",
    "    if count > min_count and token not in fr_vocab:\n",
    "        fr_vocab[token] = len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnToFrDataset(Dataset):\n",
    "    def __init__(self, data, en_vocab, fr_vocab):\n",
    "        self.enc_input = []     # 인코더의 입력 영어 데이터\n",
    "        self.dec_input = []     # 디코더의 입력 프랑스어 데이터\n",
    "        self.dec_target = []\n",
    "        for en_sent, fr_sent_in in data:\n",
    "            self.enc_input.append(en_sent)\n",
    "            self.dec_input.append(fr_sent_in)\n",
    "        self.en_vocab = en_vocab\n",
    "        self.fr_vocab = fr_vocab\n",
    "        self.max_len = 30       # 데이터가 가질 수 있는 최대 길이\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인코더와 디코더에 들어갈 샘플\n",
    "        # source, target에 대한 index sequence\n",
    "        src_sample = [self.en_vocab.get(w, self.en_vocab.get(\"[UNK]\")) for w in self.enc_input[idx]]\n",
    "        # print(src_sample)\n",
    "        trg_sample = [self.fr_vocab.get(w, self.fr_vocab.get(\"[UNK]\")) for w in self.dec_input[idx]]\n",
    "        # print(trg_sample)\n",
    "        # truncate and padding\n",
    "        src_sample = src_sample[:self.max_len]\n",
    "        trg_sample = trg_sample[:self.max_len]\n",
    "        src_sample += [self.en_vocab.get(\"[PAD]\")] * (self.max_len - len(src_sample))\n",
    "        trg_sample += [self.fr_vocab.get(\"[PAD]\")] * (self.max_len - len(trg_sample))\n",
    "\n",
    "        # dictonary 형태로, 같은 key를 가진 것끼리 batch가 만들어진다.\n",
    "        return {\"src\": torch.LongTensor(src_sample), \"trg\": torch.LongTensor(trg_sample)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EnToFrDataset(train, en_vocab, fr_vocab)\n",
    "vali_dataset = EnToFrDataset(vali, en_vocab, fr_vocab)\n",
    "test_dataset = EnToFrDataset(test, en_vocab, fr_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,drop_last=True, shuffle=True, num_workers=8)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 실습 때 사용한 Seq2Seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    # print(outputs.size())\n",
    "    # print(hidden.size())\n",
    "    # print(cell.size())\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # 인코더의 hidden, cell state를 디코더의 LSTM 입력으로\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder with simple dot product attention\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # unsqueeze : 토큰이 하나씩 들어가기 때문에 차원을 맞추기 위한 연산\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # print(output.size())\n",
    "        # print(encoder_outputs.size())\n",
    "\n",
    "        attention_score = torch.bmm(output.squeeze(0).unsqueeze(1), encoder_outputs.permute(1, 2, 0)).squeeze(1)\n",
    "        attention_distribution = torch.softmax(attention_score, dim=1)\n",
    "        context = torch.bmm(attention_distribution.unsqueeze(1), encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), context), dim=1))\n",
    "        # print(attention_score.size())\n",
    "        # print(attention_distribution.size())\n",
    "        # print(context.size())\n",
    "        # print(prediction.size())\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import lightning as pl\n",
    "\n",
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.criterian = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.automatic_optimization = False\n",
    "        # 인코더, 디코더 따로 있기 때문에 auto_optimizer를 사용할 수 없다.\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tar_len, batch_size, trg_vocab_size 만큼의 공간\n",
    "        # 디코더의 출력을 저장하는 공간\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(trg.device)\n",
    "\n",
    "        enc_output, hidden, cell = self.encoder(src)\n",
    "        # print(enc_output.size())\n",
    "        # print(hidden.size())\n",
    "        # print(cell.size())\n",
    "        \n",
    "        # 하나의 token씩 입력을 넣어준다.\n",
    "        input = trg[0,:]\n",
    "        # print(input)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            if isinstance(self.decoder, AttentionDecoder):\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
    "            else:\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "                # print(output.size())\n",
    "                # print(hidden.size())\n",
    "                # print(cell.size())\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        enc_opt, dec_opt = self.optimizers()\n",
    "\n",
    "        enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "\n",
    "        src = batch[\"src\"].permute(1, 0)    # LSTM에는 batch가 두번째로!\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.manual_backward(loss)\n",
    "        enc_opt.step()\n",
    "        dec_opt.step()\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_PPL\", math.exp(loss))   # PPL : Launguae Generation 할 때의 성능지표!\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src = batch[\"src\"].permute(1, 0)\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_PPL\", math.exp(loss))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src = batch[\"src\"].permute(1, 0)\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_PPL\", math.exp(loss))\n",
    "        return loss\n",
    "\n",
    "    # 디코더의 실제 출력을 확인\n",
    "    def decode(self, src):\n",
    "        enc_output, hidden, cell = self.encoder(src.unsqueeze(1))\n",
    "        trg_len = 30\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = [2]\n",
    "        input = torch.LongTensor([2]).to(src.device)\n",
    "        for t in range(1, trg_len):\n",
    "            if isinstance(self.decoder, AttentionDecoder):\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
    "            else:\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            outputs.append(top1.item())\n",
    "            if top1.item() == 3:\n",
    "                break\n",
    "            input = top1\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        enc_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-4)\n",
    "        dec_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=1e-4)\n",
    "        return enc_optimizer, dec_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "att_decoder = AttentionDecoder(output_dim=len(fr_vocab),\n",
    "                            emb_dim=emb_dim,\n",
    "                            hid_dim=hid_dim,\n",
    "                            n_layers=n_layers,\n",
    "                            dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "att_model = Seq2Seq(encoder, att_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoeyhesx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"basic_Seq2Seq\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240424_173054-v5v8lj1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/v5v8lj1h/workspace' target=\"_blank\">basic_Seq2Seq_epoch10</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/v5v8lj1h/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/v5v8lj1h/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | Decoder          | 12.8 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "18.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.6 M    Total params\n",
      "74.448    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2945/2945 [03:40<00:00, 13.34it/s, v_num=lj1h]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2945/2945 [03:41<00:00, 13.32it/s, v_num=lj1h]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_PPL</td><td>█▇▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▆▆▅▅▅▄▅▄▄▄▅▄▃▃▄▃▂▃▄▃▃▂▂▂▂▂▃▂▁▂▂▂▂▂▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_PPL</td><td>11.32534</td></tr><tr><td>train_loss</td><td>2.42704</td></tr><tr><td>trainer/global_step</td><td>29449</td></tr><tr><td>val_PPL</td><td>20.69061</td></tr><tr><td>val_loss</td><td>3.01041</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basic_Seq2Seq_epoch10</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/v5v8lj1h/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/v5v8lj1h/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240424_173054-v5v8lj1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ENCODER 개선 (30점)\n",
    "\n",
    "* 실습수업에 사용한 Seq-to-Seq 모델의 Encoder를 개선하시오.\n",
    "    * 합리적 이유에 기반해 개선 방법을 찾고 구현 및 실험 하시오\n",
    "        * 여러 제약사항(컴퓨팅, 메모리 등)이 있으므로 꼭 성능이 높아져야 하는 것은 아님\n",
    "    * 왜 그런 모델 구성을 생각했는지, 그 결과가 어떻게 나타났는지 기술하시오\n",
    "        * 성능이 높아졌다면 왜 그렇다고 생각하는지, 낮아졌다면 무엇이 문제인 것 같은지\n",
    "\n",
    "    * Hint\n",
    "        * 꼭 Encoder의 구조가 RNN 계열의 모델이어야 하는가?\n",
    "        * Bi-directional RNN을 사용한다면?\n",
    "        * ...\n",
    "\n",
    "**GRADING**\n",
    "* 적용한 방법 1개당 (+15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Encoder 개선 - BiLSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, bidirectional=True)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    # print(outputs.size())\n",
    "    # print(hidden.size())\n",
    "    # print(cell.size())\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 64, 1024])\n",
      "torch.Size([4, 64, 512])\n",
      "torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                emb_dim=emb_dim,\n",
    "                hid_dim=hid_dim,\n",
    "                n_layers=n_layers,\n",
    "                dropout=0.5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1, 0)\n",
    "    encoder.forward(src=src)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder의 LSTM 모델을 Bi-directional LSTM으로 변경\n",
    "- 양방향 문맥을 파악하면 더 효과적일 것이라 생각!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # biLSTM으로 인해 hidden_state output이 2배가 되어 num_layers도 2배로 설정\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers*2, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # print(input.size())\n",
    "        input = input.unsqueeze(0)\n",
    "        # print(input.size())\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # 인코더의 hidden, cell state를 디코더의 LSTM 입력으로\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # print(prediction.size())\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1, 0)\n",
    "    trg = batch[\"trg\"].permute(1, 0)\n",
    "    enc_output, hidden, cell = encoder(src)\n",
    "    decoder.forward(input=trg[0], hidden=hidden, cell=cell)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Encoder 개선 Test - BiLSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_biLSTM_enc\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240424_150815-43kk0z0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/43kk0z0r/workspace' target=\"_blank\">Seq2Seq_biLSTM_enc</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/43kk0z0r/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/43kk0z0r/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 11.6 M\n",
      "1 | decoder   | Decoder          | 17.0 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "28.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.6 M    Total params\n",
      "114.333   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [05:20<00:00,  9.19it/s, v_num=0z0r]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [05:21<00:00,  9.17it/s, v_num=0z0r]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▅▅▄▃▄▃▃▃▃▂▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▆▅▅▅▅▅▄▄▄▃▄▄▄▄▃▃▄▃▃▂▃▃▃▂▃▂▂▁▂▂▃▁▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>59.07238</td></tr><tr><td>train_loss</td><td>4.07876</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>59.90458</td></tr><tr><td>val_loss</td><td>4.08314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_biLSTM_enc</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/43kk0z0r/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/43kk0z0r/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240424_150815-43kk0z0r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Encoder 개선 - Encoder의 입력을 반전시키자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    # print(src)\n",
    "    src = torch.stack([seq.flip(0) for seq in src])\n",
    "    # print(src)\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 165,  447,  178,  ...,   31,  742,   33],\n",
      "        [ 287,  278,   33,  ...,   44,  151,   44],\n",
      "        [  14,   12, 1959,  ...,   14,   12,  139],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]])\n",
      "tensor([[  33,  742,   31,  ...,  178,  447,  165],\n",
      "        [  44,  151,   44,  ...,   33,  278,  287],\n",
      "        [ 139,   12,   14,  ..., 1959,   12,   14],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                emb_dim=emb_dim,\n",
    "                hid_dim=hid_dim,\n",
    "                n_layers=n_layers,\n",
    "                dropout=0.5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1, 0)\n",
    "    encoder.forward(src=src)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Encoder 개선 Test - Encoder의 입력을 반전시키자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_reverse_input\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240430_210611-zmnrn13o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/zmnrn13o/workspace' target=\"_blank\">Seq2Seq_reverse_input</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/zmnrn13o/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/zmnrn13o/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | Decoder          | 12.8 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "18.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.6 M    Total params\n",
      "74.479    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [03:43<00:00, 13.20it/s, v_num=n13o]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [03:43<00:00, 13.18it/s, v_num=n13o]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▄▅▄▃▃▃▂▂▃▂▂▃▂▃▁▂▂▂▂▃▂▂▂▂▁▂▃▁▂▂▁▁▁▁▃▂▂▂▁</td></tr><tr><td>train_loss</td><td>█▅▆▅▄▄▄▃▂▃▃▃▃▃▄▂▃▂▃▃▃▃▃▂▃▁▃▄▁▂▂▁▂▁▁▄▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>130.14435</td></tr><tr><td>train_loss</td><td>4.86864</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>178.80872</td></tr><tr><td>val_loss</td><td>5.18042</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_reverse_input</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/zmnrn13o/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/zmnrn13o/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240430_210611-zmnrn13o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DECODER 개선 (30점)\n",
    "\n",
    "* 실습수업에 사용한 Seq-to-Seq 모델의 Decoder를 개선하시오.\n",
    "    * 합리적 이유에 기반해 개선 방법을 찾고 구현 및 실험 하시오\n",
    "        * 여러 제약사항(컴퓨팅, 메모리 등)이 있으므로 꼭 성능이 높아져야 하는 것은 아님\n",
    "    * 왜 그런 모델 구성을 생각했는지, 그 결과가 어떻게 나타났는지 기술하시오\n",
    "        * 성능이 높아졌다면 왜 그렇다고 생각하는지, 낮아졌다면 무엇이 문제인 것 같은지\n",
    "\n",
    "    * Hint\n",
    "        * 최종 output을 만들 떄 마지막 layer의 hidden vector만 사용하는게 최선인가? 이전 layer의 hidden vector도 같이 사용한다면?\n",
    "        * 왜 encoder와 decoder의 크기 차이가 많이 발생하는가? 이를 해결할 수 없는가?\n",
    "        * 현재 Encoder의 마지막 hidden vector를 사용하는데 대부분 [PAD] 토큰이다. [PAD] 토큰의 hidden vector를 사용하는게 맞는가?\n",
    "        * ...\n",
    "\n",
    "**GRADING**\n",
    "* 적용한 방법 1개당 (+15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Decoder 개선 - Decoder의 크기를 줄여보자!\n",
    "\n",
    "- Encoder와 Decoder의 모델 크기를 확인해보면 각각 `5.8M` `12.8M` 으로 꽤 차이나 난다.\n",
    "- 두개의 구조 상 결정적인 차이는 출력층에서 Fully Conntected Layer를 사용한다는 것이다.\n",
    "- Decoder의 Embedding, Linear가 같은 파라미터를 공유하면 모델 크기를 줄일 수 있지 않을까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        # 차원을 맞추기 위한 Projection Layer\n",
    "        self.projection = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim, bias=False)\n",
    "        self.fc_out.weight = self.embedding.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # print(output.size())\n",
    "        output = self.projection(output.squeeze(0))\n",
    "        # print(output.size())\n",
    "        prediction = self.fc_out(output)\n",
    "        # print(prediction.size())\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                 emb_dim=emb_dim,\n",
    "                 hid_dim=hid_dim,\n",
    "                 n_layers=n_layers,\n",
    "                 dropout=0.5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1, 0)\n",
    "    trg = batch[\"trg\"].permute(1, 0)\n",
    "    enc_output, hidden, cell = encoder(src)\n",
    "    decoder.forward(input=trg[0], hidden=hidden, cell=cell)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Decoder 개선 Test - Decoder의 크기를 줄여보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_decoder_weight_tying\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240424_165409-gyjrp2el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/gyjrp2el/workspace' target=\"_blank\">Seq2Seq_decoder_weight_tying_epoch10</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/gyjrp2el/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/gyjrp2el/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | Decoder          | 6.8 M \n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "12.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.7 M    Total params\n",
      "50.635    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2945/2945 [03:25<00:00, 14.35it/s, v_num=p2el]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2945/2945 [03:25<00:00, 14.34it/s, v_num=p2el]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_PPL</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▄▄▄▄▃▃▃▃▃▃▃▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_PPL</td><td>14.53211</td></tr><tr><td>train_loss</td><td>2.67636</td></tr><tr><td>trainer/global_step</td><td>29449</td></tr><tr><td>val_PPL</td><td>27.54312</td></tr><tr><td>val_loss</td><td>3.29282</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_decoder_weight_tying_epoch10</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/gyjrp2el/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/gyjrp2el/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240424_165409-gyjrp2el/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 공유를 통해 `5.8M` `6.9M` 으로 Encoder Decoder의 모델 크기 차이를 크게 감소시켰다.\n",
    "- 학습 과정에서 Loss와 PPL은 조금 더 높게 나오긴 했지만 비슷한 수치이며 모델을 경량화하면서 비슷한 성능을 보인다는 것은 유의미한 결과라고 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Decoder 개선 - [PAD] 토큰을 무시하자!\n",
    "- PAD 토큰의 역할은 가변 길이 시퀀스를 고정 길이 matrix로 만들어 병렬 처리가 가능하도록 해준다.\n",
    "- 하지만, PAD 토큰으로 인해 Encoder의 마지막 hidden vector는 대부분 [PAD] 토큰이다.\n",
    "- 따라서, Encoder에서 [PAD] 토큰에 마스킹을 해서 의미없는 토큰인 PAD 토큰을 무시하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    mask = (src == 0)\n",
    "    lengths = mask.logical_not().sum(dim=0)\n",
    "\n",
    "    masked = pack_padded_sequence(embedded, lengths.cpu(), batch_first=False, enforce_sorted=False)\n",
    "    outputs, (hidden, cell) = self.rnn(masked)\n",
    "\n",
    "    outputs, _ = pad_packed_sequence(outputs, batch_first=False)\n",
    "    \n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1, 0)\n",
    "    trg = batch[\"trg\"].permute(1, 0)\n",
    "    output, hidden, cell = encoder.forward(src=src)\n",
    "    decoder.forward(trg[0,:], hidden, cell)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Decoder 개선 Test - [PAD] 토큰을 무시하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                    emb_dim=emb_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_masked_pad_token\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | Decoder          | 12.8 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "18.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.6 M    Total params\n",
      "74.394    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [03:29<00:00, 14.05it/s, v_num=dyxd]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [03:29<00:00, 14.03it/s, v_num=dyxd]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▅▅▅▄▄▄▄▃▄▃▃▃▄▂▃▃▃▃▂▂▃▂▂▂▂▃▂▂▂▂▂▂▁▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>60.19929</td></tr><tr><td>train_loss</td><td>4.09766</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>67.88832</td></tr><tr><td>val_loss</td><td>4.20872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_masked_pad_token</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/64nndyxd/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/64nndyxd/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240429_234155-64nndyxd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ENC-DEC 개선 (40점)\n",
    "\n",
    "* 실습수업에 사용한 Seq-to-Seq 모델의 Encoder-Decoder의 연결부분을 개선하시오.\n",
    "    * 합리적 이유에 기반해 개선 방법을 찾고 구현 및 실험 하시오\n",
    "        * 여러 제약사항(컴퓨팅, 메모리 등)이 있으므로 꼭 성능이 높아져야 하는 것은 아님\n",
    "    * 왜 그런 모델 구성을 생각했는지, 그 결과가 어떻게 나타났는지 기술하시오\n",
    "        * 성능이 높아졌다면 왜 그렇다고 생각하는지, 낮아졌다면 무엇이 문제인 것 같은지\n",
    "\n",
    "    * Hint\n",
    "        * Attention을 개선할 수 없을까? (Dot attention을 QKV attention으로 개선, weighted attention 등)\n",
    "        * Enc-DEC의 layer 수가 다른 경우는 어떻게 처리할 것인가?\n",
    "        * ...\n",
    "\n",
    "\n",
    "**GRADING**\n",
    "* 적용한 방법 1개당 (+15) (최대 40점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) ENC-DEC 개선 - QKV attention\n",
    "- 기존 단순한 Dot attention을 QKV attention으로 개선해본다.\n",
    "- query는 Decoder의 output, key&value는 Encoder outputs\n",
    "- 각각 Projection 연산 후에 attention score를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.Wq = nn.Linear(hid_dim, hid_dim)\n",
    "        self.Wk = nn.Linear(hid_dim, hid_dim)\n",
    "        self.Wv = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        query = self.Wq(output)\n",
    "        key = self.Wk(encoder_outputs)\n",
    "        value = self.Wv(encoder_outputs)\n",
    "\n",
    "        attention_score = torch.bmm(query.squeeze(0).unsqueeze(1), key.permute(1, 2, 0)).squeeze(1)\n",
    "        attention_distribution = torch.softmax(attention_score, dim=1)\n",
    "        context = torch.bmm(attention_distribution.unsqueeze(1), value.permute(1, 0, 2)).squeeze(1)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), context), dim=1))\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) ENC-DEC 개선 Test - QKV attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "att_decoder = AttentionDecoder(output_dim=len(fr_vocab),\n",
    "                            emb_dim=emb_dim,\n",
    "                            hid_dim=hid_dim,\n",
    "                            n_layers=n_layers,\n",
    "                            dropout=0.5)\n",
    "\n",
    "att_model = Seq2Seq(encoder, att_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_QKV_att\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240429_214335-x7bgyk8q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/x7bgyk8q/workspace' target=\"_blank\">Seq2Seq_QKV_att</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/x7bgyk8q/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/x7bgyk8q/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | AttentionDecoder | 19.6 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "25.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.5 M    Total params\n",
      "101.815   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [06:04<00:00,  8.07it/s, v_num=yk8q]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [06:05<00:00,  8.06it/s, v_num=yk8q]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▅▆▅▄▄▅▅▄▅▄▄▅▄▄▄▃▂▃▂▂▂▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>train_loss</td><td>█▆▇▆▆▆▇▆▆▆▆▆▇▅▅▅▅▄▄▄▃▄▄▅▃▃▄▄▃▃▄▃▃▃▃▃▂▁▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>78.2801</td></tr><tr><td>train_loss</td><td>4.36029</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>76.56445</td></tr><tr><td>val_loss</td><td>4.32845</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_QKV_att</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/x7bgyk8q/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/x7bgyk8q/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240429_214335-x7bgyk8q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(att_model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Enc-Dec 개선 - Multi-Head Attention\n",
    "- Multi-Head attention의 이점은 다음과 같다.\n",
    "    - 입력 시퀀스의 다양한 표현을 병렬로 학습할 수 있다.\n",
    "    - 그로 인해 표현력이 높아질 수 있다.\n",
    "- `nn.MultiheadAttention` 모듈을 활용해 이를 적용해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.Wq(query)\n",
    "        key = self.Wk(key)\n",
    "        value = self.Wv(value)\n",
    "        \n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attn = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(self.depth)\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, value)\n",
    "        \n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        out = out.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        return self.dense(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hid_dim, num_heads=8, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        att_output, _ = self.mha(output, encoder_outputs, encoder_outputs)\n",
    "        outputs = torch.cat((output.squeeze(1), att_output.squeeze(1)), dim=2)\n",
    "\n",
    "        prediction = self.fc_out(self.dropout(outputs)).squeeze(0)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Enc-Dec 개선 Test - Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=n_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "att_decoder = AttentionDecoder(output_dim=len(fr_vocab),\n",
    "                               emb_dim=emb_dim,\n",
    "                               hid_dim=hid_dim,\n",
    "                               n_layers=n_layers,\n",
    "                               dropout=0.5)\n",
    "\n",
    "att_model = Seq2Seq(encoder, att_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_multi_head_att\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240429_225733-i0jpjnps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/i0jpjnps/workspace' target=\"_blank\">Seq2Seq_multi_head_att</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/i0jpjnps/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/i0jpjnps/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | AttentionDecoder | 19.9 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "25.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.7 M    Total params\n",
      "102.865   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [06:18<00:00,  7.77it/s, v_num=jnps]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [06:19<00:00,  7.77it/s, v_num=jnps]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▆▆▅▄▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▁▂▂▂▁▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▇▆▅▅▅▅▅▅▄▄▄▄▃▄▄▃▃▄▄▃▄▃▃▃▁▂▃▃▂▃▃▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>43.44275</td></tr><tr><td>train_loss</td><td>3.77144</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>62.35681</td></tr><tr><td>val_loss</td><td>4.12248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_multi_head_att</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/i0jpjnps/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/i0jpjnps/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240429_225733-i0jpjnps/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(att_model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Enc-Dec 개선 - Encoder와 Decoder의 Layer 수가 다를땐?\n",
    "- 보통 Encoder와 Decoder의 레이어 수를 같게 하지만 다른 경우 어떻게 처리할 것인지에 대해 고민해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embedding(src))\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import lightning as pl\n",
    "\n",
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.criterian = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.automatic_optimization = False\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(trg.device)\n",
    "\n",
    "        enc_output, hidden, cell = self.encoder(src)\n",
    "        # print(hidden.size())\n",
    "        # print(cell.size())\n",
    "        \n",
    "        if self.encoder.rnn.num_layers > self.decoder.rnn.num_layers:\n",
    "            hidden = hidden[-self.decoder.rnn.num_layers:]\n",
    "            cell = cell[-self.decoder.rnn.num_layers:]   \n",
    "        else:\n",
    "            diff = (self.decoder.rnn.num_layers // self.encoder.rnn.num_layers)\n",
    "            hidden = torch.cat([hidden] * diff, dim=0)\n",
    "            cell = torch.cat([cell] * diff, dim=0)\n",
    "        # print(hidden.size())\n",
    "        # print(cell.size())\n",
    "\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            if isinstance(self.decoder, AttentionDecoder):\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
    "            else:\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        enc_opt, dec_opt = self.optimizers()\n",
    "\n",
    "        enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "\n",
    "        src = batch[\"src\"].permute(1, 0)\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.manual_backward(loss)\n",
    "        enc_opt.step()\n",
    "        dec_opt.step()\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_PPL\", math.exp(loss))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src = batch[\"src\"].permute(1, 0)\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_PPL\", math.exp(loss))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src = batch[\"src\"].permute(1, 0)\n",
    "        trg = batch[\"trg\"].permute(1, 0)\n",
    "\n",
    "        outputs = self(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        outputs = outputs[1:].view(-1, outputs_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = self.criterian(outputs, trg)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_PPL\", math.exp(loss))\n",
    "        return loss\n",
    "    \n",
    "    def decode(self, src):\n",
    "        enc_output, hidden, cell = self.encoder(src.unsqueeze(1))\n",
    "        trg_len = 30\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = [2]\n",
    "        input = torch.LongTensor([2]).to(src.device)\n",
    "        for t in range(1, trg_len):\n",
    "            if isinstance(self.decoder, AttentionDecoder):\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell, enc_output)\n",
    "            else:\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            outputs.append(top1.item())\n",
    "            if top1.item() == 3:\n",
    "                break\n",
    "            input = top1\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        enc_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-4)\n",
    "        dec_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=1e-4)\n",
    "        return enc_optimizer, dec_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "enc_layers = 2\n",
    "dec_layers = 4\n",
    "\n",
    "encoder = Encoder(input_dim=len(en_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=enc_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "decoder = Decoder(output_dim=len(fr_vocab),\n",
    "                  emb_dim=emb_dim,\n",
    "                  hid_dim=hid_dim,\n",
    "                  n_layers=dec_layers,\n",
    "                  dropout=0.5)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 512])\n",
      "torch.Size([2, 64, 512])\n",
      "torch.Size([4, 64, 512])\n",
      "torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    src = batch[\"src\"].permute(1,0)\n",
    "    trg = batch[\"trg\"].permute(1,0)\n",
    "    model.forward(src=src, trg=trg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"NLP\", name=\"Seq2Seq_enc2_dec4\", group=\"HW03\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240430_004527-limz511f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/limz511f/workspace' target=\"_blank\">Seq2Seq_enc2_dec4</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/limz511f/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/limz511f/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 5.8 M \n",
      "1 | decoder   | Decoder          | 17.0 M\n",
      "2 | criterian | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "22.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.8 M    Total params\n",
      "91.063    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [04:40<00:00, 10.50it/s, v_num=511f]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2945/2945 [04:40<00:00, 10.49it/s, v_num=511f]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_PPL</td><td>█▄▄▃▅▃▄▄▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▁▂▁</td></tr><tr><td>train_loss</td><td>█▆▅▅▆▅▅▅▅▄▅▅▄▃▄▅▄▃▃▃▂▃▃▃▄▂▂▃▂▃▁▂▂▂▂▁▃▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_PPL</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_PPL</td><td>82.6241</td></tr><tr><td>train_loss</td><td>4.4143</td></tr><tr><td>trainer/global_step</td><td>2944</td></tr><tr><td>val_PPL</td><td>94.55687</td></tr><tr><td>val_loss</td><td>4.54029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Seq2Seq_enc2_dec4</strong> at: <a href='https://wandb.ai/noeyhesx/NLP/runs/limz511f/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/limz511f/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240430_004527-limz511f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, vali_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
