# NLP Assignment02
**ì‘ì„±ì** : `2019136056 ë°•ì„¸í˜„`
**ì œì¶œì¼** : `2024/04/09`

## 1. IMDB ë°ì´í„° ì²˜ë¦¬ (20ì )

- stanford ëŒ€í•™ì—ì„œ ì œê³µí•˜ëŠ” IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°(https://ai.stanford.edu/~amass/data/sentiment/)ë¥¼ ë‹¤ìš´ ë°›ì•„ í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´ëŸ¬ë¥¼ êµ¬ì„±í•˜ì‹œì˜¤.
	- ë°ì´í„°ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê¸ì •/ë¶€ì •ì˜ Binary Classification ë°ì´í„°ì…‹ì„
	- ë°ì´í„°ì…‹ì˜ ì••ì¶•ì„ í•´ì œí–ˆì„ ë•Œì˜ ê° ë””ë ‰í† ë¦¬ì˜ ìš©ë„ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.
		- train/pos : ê¸ì • labelì˜ í•™ìŠµ ë°ì´í„°
		- train/neg : ë¶€ì • labelì˜ í•™ìŠµ ë°ì´í„°
		- test/pos : ê¸ì • labelì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°
		- test/neg : ë¶€ì • labelì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°
	- ì§€ê¸ˆê» ë°°ìš´ ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì ìš©í•´ tokenizing, nomalizing ë“±ì„ ì§„í–‰í•œ í›„ vocabì„ êµ¬ì¶•í•˜ì—¬ì•¼í•¨.

**GRADING**
- ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ë¥¼ í†µí•´ vocabì„ êµ¬ì¶• (+20)

### 1.1 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

```shell
!tar -xzf /content/aclImdb_v1.tar.gz
```
- `aclImdb_v1.tar.gz` íŒŒì¼ì„ Colabì— ì—…ë¡œë“œí•˜ì—¬ ì••ì¶•í•´ì œ í›„ ì „ì²˜ë¦¬ ì½”ë“œë¥¼ ì ìš©í•œë‹¤.

```python
from pathlib import Path  
  
raw_data_dir = './aclImdb'  
data_dir = Path(raw_data_dir)  
  
train_datas = []  
test_datas = []  
  
for sentiment in ["pos", "neg"]:  
    samples = list(data_dir.glob(f"train/{sentiment}/*.txt"))  
    train_datas.extend(samples)  
  
for sentiment in ["pos", "neg"]:  
    samples = list(data_dir.glob(f"test/{sentiment}/*.txt"))  
    test_datas.extend(samples)  
  
train_file = open("train.txt", "w", encoding="utf-8")  
test_file = open("test.txt", "w", encoding="utf-8")  
  
for file, datas in [(train_file, train_datas), (test_file, test_datas)]:  
    file.write("id\ttext\tlabel\n")  
    for data in datas:  
        lines = [line.strip().replace("\t", " ") for line in data.open().readlines()]  
        text = " ".join(lines)  
        id = data.name[:-4]  
        label = 1 if "pos" in data.parts else 0  
        file.write(f"{id}\t{text}\t{label}\n")  
      
train_file.close()  
test_file.close()
```

- ë‹¤ìš´ë¡œë“œ ë°›ì€ ë°ì´í„°ì…‹ì˜ í´ë” ê²½ë¡œë¥¼ ì €ì¥í•˜ê³  `pos, neg` í´ë”ì— ì ‘ê·¼í•´ `train_datas, test_datas` ë¥¼ ë§Œë“ ë‹¤.
- `train.txt, test.txt` íŒŒì¼ë¡œ í˜•ì‹ì„ ë§ì¶° ì €ì¥í•œë‹¤.
	- ì´ë•Œ, `\t` ì„ ì œê±°í•œ ì´ìœ ëŠ” í›„ì— ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì—ì„œ ëª‡ê°œì˜ ë°ì´í„°ì— ì˜í•œ ë¬¸ì œê°€ ë°œìƒí–ˆê³ , ê·¸ ì›ì¸ì´ í…ìŠ¤íŠ¸ ì¤‘ê°„ì— íƒ­ ë¬¸ìê°€ ë“¤ì–´ê°€ ìˆì—ˆê¸° ë•Œë¬¸ì´ì—ˆë‹¤.
	- ë”°ë¼ì„œ `\t` ë¬¸ìë¥¼ ì‚¬ì „ì— ì œê±°í•˜ì˜€ë‹¤.

```python
with open("train.txt", "r", encoding="utf-8") as file:  
    contents = file.read()  
    lines = contents.split("\n")[1:]  
    train_data = [line.split("\t") for line in lines if len(line) > 0]  
  
with open("test.txt", "r", encoding="utf-8") as file:  
    contents = file.read()  
    lines = contents.split("\n")[1:]  
    test_data = [line.split("\t") for line in lines if len(line) > 0]
```

### 1.2 Vocabulary êµ¬ì¶•

```python
import nltk  
  
nltk.download('punkt')  
nltk.download('stopwords')
```

```python
import re  
from tqdm import tqdm  
from nltk.tokenize import word_tokenize  
from nltk.corpus import stopwords  
from nltk.stem import PorterStemmer  
  
stop_words = set(stopwords.words('english'))  
stemmer = PorterStemmer()  
  
tokenized_train_dataset = []  
tokenized_test_dataset = []  
  
for data in tqdm(train_data):  
    text = data[1].lower().replace('<br />', '')  # Remove <br /> tags and lowercase  
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation  
    tokens = word_tokenize(text)  
    tokens = [word for word in tokens if word not in stop_words]  
    tokens = [stemmer.stem(word) for word in tokens]  
    labels = data[2]  
    tokenized_train_dataset.append((tokens, labels))  
  
for data in tqdm(test_data):  
    text = data[1].lower().replace('<br />', '')  # Remove <br /> tags and lowercase  
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation  
    tokens = word_tokenize(text)  
    tokens = [word for word in tokens if word not in stop_words]  
    tokens = [stemmer.stem(word) for word in tokens]  
    labels = data[2]  
    tokenized_test_dataset.append((tokens, labels))
```

- ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
	- ë°ì´í„°ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ë©° `<br />` íƒœê·¸ë¥¼ ì œê±°í•œë‹¤.
	- ë¬¸ì¥ë¶€í˜¸ ë° íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
	- **word_tokenize** ë¥¼ ì´ìš©í•´ tokenizing í•œë‹¤.
	- `ë¶ˆìš©ì–´` ë¥¼ ì œê±°í•œë‹¤.
	- **PorterStemmer** ë¥¼ ì´ìš©í•´ stemmingì„ í•œë‹¤.

```python
from collections import Counter  
  
token_counter = Counter()  
  
for tokens, _ in tokenized_train_dataset:  
    token_counter.update(tokens)  
  
min_count = 2  
vocab = {"[PAD]":0, "[UNK]":1}  
vocab_idx = 2  
  
for token, count in token_counter.items():  
    if count > min_count:  
        vocab[token] = vocab_idx  
        vocab_idx += 1
```

- 2ë²ˆ ì´í•˜ë¡œ ë“±ì¥í•˜ëŠ” í† í°ì€ ì œì™¸í•˜ê³  **vocab** ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.

![](../../assets/HW02/result01.png)

- ìì£¼ ë“±ì¥í•˜ëŠ” í† í°ê³¼ ê·¸ë ‡ì§€ ì•Šì€ í† í°ì„ ì¶œë ¥í•œ ê²°ê³¼ëŠ” ìœ„ì™€ ê°™ë‹¤.
- `Stemming` ì„ ì ìš©í–ˆê¸° ë•Œë¬¸ì— **movi, charact** ì™€ ê°™ì€ í˜•íƒœë¡œ ë‹¨ì–´ê°€ ë³€ê²½ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
word2vec_train_datas = []  
for train_text, _ in tokenized_train_dataset:  
    word2vec_train_datas.append([word for word in train_text])
```

```python
from gensim.models import Word2Vec  
  
SkipGram_W2V = Word2Vec(sentences = word2vec_train_datas, vector_size = 200, window = 5, min_count = 1, workers = 4, sg = 1)
```

- `vector_size` ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 100~300 ì‚¬ì´ë¡œ ê²°ì •í•œë‹¤ê³  í•œë‹¤.
- ë°ì´í„°ì…‹ì˜ ë³µì¡ì„±, vocabì˜ í¬ê¸° ë“± ì—¬ëŸ¬ ìš”ì†Œë¥¼ ê³ ë ¤í•´ ê²°ì •ì„ í•˜ëŠ”ë° ì•„ì§ ê²½í—˜ì´ ë¶€ì¡±í•˜ë¯€ë¡œ ì‹¤í—˜ì„ í†µí•´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” `vector_size` ë¥¼ ì„ íƒí•˜ê¸°ë¡œ í–ˆë‹¤.
	- `[32, 100, 200, 300]` ì„ ë™ì¼í•œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ í–ˆì„ ë•Œ, ë‚˜ì˜ ê²½ìš° **vector_size = 200** ì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
	- ë”°ë¼ì„œ, ì´ Taskì—ì„œ `vector_size` ëŠ” **200** ìœ¼ë¡œ ê²°ì •í•œë‹¤.
- ![](../../assets/HW02/gpt01.png)
- ![](../../assets/HW02/gpt02.png)

```python
import numpy as np  
  
embedding_list = []  
  
for token, idx in vocab.items():  
    if token in SkipGram_W2V.wv:  
        embedding_list.append(SkipGram_W2V.wv[token])  
    elif token == "[PAD]":  
        embedding_list.append(np.zeros(SkipGram_W2V.wv.vectors.shape[1]))  
    elif token == "[UNK]":  
        embedding_list.append(np.random.uniform(-1, 1, SkipGram_W2V.wv.vectors.shape[1]))  
    else:  
        embedding_list.append(np.random.uniform(-1, 1, SkipGram_W2V.wv.vectors.shape[1]))  
  
embedding_lookup_matrix = np.vstack(embedding_list)  
  
print(embedding_lookup_matrix.shape)  
print(len(vocab))
```

![](../../assets/HW02/result02.png)

---

## 2. ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„ (30ì )

- 1ì—ì„œ ì²˜ë¦¬í•œ vocabì„ í†µí•´ tokenizing ëœ ë°ì´í„°ì…‹ì˜ ì—¬ëŸ¬ í†µê³„ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.
	- í†µê³„ì˜ ì˜ˆì‹œ
		- í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¬¸ì„œì˜ ìˆ˜
		- í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í‰ê·  token ìˆ˜
		- ë°ì´í„°ì˜ token histogram
		- í•™ìŠµ/í…ŒìŠ¤íŠ¸ì—ì„œì˜ UNK tokenì˜ ìˆ˜
		- ê° tokenì˜ ë¹ˆë„ ê·¸ë˜í”„
		- ê¸ì •/ë¶€ì •ì˜ token ë¹ˆë„ ì°¨ì´
		- ê¸ì •/ë¶€ì •ì˜ frequent/rare token
- ì´ì „ ì‹¤ìŠµê¹Œì§€ ì‚¬ìš©í•œ ì½”ë“œ ë° ê²€ìƒ‰ì„ í™œìš©í•´ ìµœì†Œ 1ê°œì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì•¼ í•¨.

**GRADING**
- ë¶„ì„í•œ í†µê³„ì˜ ìˆ˜ (+5)

```python
import matplotlib.pyplot as plt  
from collections import Counter
```

### 2.1 í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¬¸ì„œì˜ ìˆ˜

```python
print('Train Document : ', len(tokenized_train_dataset))  
print('Test Document : ', len(tokenized_test_dataset))
```

![](../../assets/HW02/result03.png)

### 2.2 í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í‰ê·  token ìˆ˜

```python
train_avg_tokens = sum(map(lambda x: len(x[0]), tokenized_train_dataset))
							/ len(tokenized_train_dataset)  
test_avg_tokens = sum(map(lambda x: len(x[0]), tokenized_test_dataset))
							/ len(tokenized_test_dataset)  
  
print('í•™ìŠµ ë°ì´í„°ì˜ í‰ê·  token ìˆ˜: ', train_avg_tokens)  
print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í‰ê·  token ìˆ˜: ', test_avg_tokens)
```

![](../../assets/HW02/result04.png)
### 2.3 ë°ì´í„°ì˜ token histogram

```python
# ë°ì´í„°ì˜ token histogram
train_tokens = [len(tokens) for tokens, _ in tokenized_train_dataset]
test_tokens = [len(tokens) for tokens, _ in tokenized_test_dataset]

# Train
plt.hist(train_tokens, bins=100)
plt.title('Training Data Token Histogram')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.axvline(x=train_avg_tokens, color='red', linestyle='--', label='Average')
plt.legend()
plt.show()

# Test
plt.hist(test_tokens, bins=100)
plt.title('Test Data Token Histogram')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.axvline(x=train_avg_tokens, color='red', linestyle='--', label='Average')
plt.legend()
plt.show()
```

![](../../assets/HW02/result05.png)

![](../../assets/HW02/result06.png)

### 2.4 í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ UNK token ìˆ˜

```python
UNK_token_train = sum(1 for tokens, _ in tokenized_train_dataset for token in tokens if token not in vocab)  
UNK_token_test = sum(1 for tokens, _ in tokenized_test_dataset for token in tokens if token not in vocab)  
  
print('í•™ìŠµ ë°ì´í„°ì—ì„œ UNK tokenì˜ ìˆ˜: ', UNK_token_train)  
print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ UNK tokenì˜ ìˆ˜: ', UNK_token_test)
```

![](../../assets/HW02/result07.png)
### 2.5 ê° tokenì˜ ë¹ˆë„ ê·¸ë˜í”„

```python
# ê° tokenì˜ ë¹ˆë„ ê·¸ë˜í”„
train_token_counter = Counter(train_tokens)
test_token_counter = Counter(test_tokens)

plt.figure(figsize=(20, 10))  
plt.subplot(1, 2, 1)  
plt.bar(*zip(*train_token_counter.most_common(20)))  
plt.title("Top 20 Tokens in Train Data")  
plt.xlabel("Token")  
plt.ylabel("Frequency")  
plt.xticks(rotation=45)
plt.subplot(1, 2, 2)  
plt.bar(*zip(*test_token_counter.most_common(20)))  
plt.title("Top 20 Tokens in Test Data")  
plt.xlabel("Token")  
plt.ylabel("Frequency")  
plt.xticks(rotation=45)  
plt.tight_layout()  
plt.show()
```

![](../../assets/HW02/result08.png)
### 2.6 ê¸ì •/ë¶€ì • tokenì˜ ë¹ˆë„ ì°¨ì´

```python
pos_train_tokens = [token for tokens, label in tokenized_train_dataset if label == '1' for token in tokens]  
neg_train_tokens = [token for tokens, label in tokenized_train_dataset if label == '0' for token in tokens]  
  
pos_token_counter = Counter(pos_train_tokens)  
neg_token_counter = Counter(neg_train_tokens)  
  
print('ê¸ì • token ìˆ˜: ', len(pos_train_tokens))  
print('ë¶€ì • token ìˆ˜: ', len(neg_train_tokens))  
  
plt.figure(figsize=(12, 6))  
plt.bar(*zip(*pos_token_counter.most_common(20)), color='b', alpha=0.5, label='Positive')  
plt.bar(*zip(*neg_token_counter.most_common(20)), color='r', alpha=0.5, label='Negative')  
plt.title("Top 20 Tokens by Sentiment")  
plt.xlabel("Token")  
plt.ylabel("Frequency")  
plt.xticks(rotation=45)  
plt.legend()  
plt.tight_layout()  
plt.show()
```

![](../../assets/HW02/result09.png)
### 2.7 ê¸ì •/ë¶€ì •ì˜ frequent/rare token

```python
# ê¸ì •/ë¶€ì •ì˜ frequent/rare token
pos_frequent_tokens = pos_token_counter.most_common(10)
pos_rare_tokens = pos_token_counter.most_common()[-10:]

neg_frequent_tokens = neg_token_counter.most_common(10)
neg_rare_tokens = neg_token_counter.most_common()[-10:]

print('ê¸ì • frequent token: ', pos_frequent_tokens)
print('ê¸ì • rare token: ', pos_rare_tokens)
print('ë¶€ì • frequent token: ', neg_frequent_tokens)
print('ë¶€ì • rare token: ', neg_rare_tokens)
```

![](../../assets/HW02/result10.png)

---

## 3. Classification ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ (50ì )

- ì´ë¡  ë° ì‹¤ìŠµ ìˆ˜ì—…ì„ í†µí•´ ë°°ìš´ MLP, CNN, RNNì„ ì‚¬ìš©í•˜ì—¬ ê°ìì˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì‹œì˜¤.
	- ëª¨ë¸ì˜ í¬ê¸°ëŠ” ModelSummary ê¸°ì¤€ 500MBì˜ ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•˜ë©´ ì•ˆë¨.
	- ëª¨ë¸ì€ ìµœëŒ€ 10 epoch í•™ìŠµ í•  ìˆ˜ ìˆìŒ (ì ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒì€ ok)
- ìµœëŒ€í•œ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ì—¬ì•¼ í•¨.
	- í•™ìŠµì— ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ì•¼ í•¨.
	- í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ë©´ 0ì 
	- ë‹¨, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í†µê³„ë¥¼ ë³´ê³  ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ê°€ëŠ¥!
- ëª¨ë¸ êµ¬ì„±ì— ìˆì–´ ì™œ ìì‹ ì´ ê·¸ëŸ° ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„ í•˜ì˜€ëŠ”ì§€ ì„¤ëª…ì„ í•˜ì—¬ì•¼í•¨.

**GRADING**
- ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ (+20)
- ëª¨ë¸ì— ëŒ€í•œ ì„¤ëª… (+10)
- ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¥¸ ì„±ì 
	- ìƒìœ„ 0~30% : +20
	- ìƒìœ„ 30~50% : +15
	- ìƒìœ„ 50~70% : +10
	- ìƒìœ„ 70~100% : +5

### 3.1 Padding

í…ìŠ¤íŠ¸ëŠ” ë°ì´í„° ë³„ë¡œ ê¸¸ì´ê°€ ë‹¤ë¥´ë‹¤.
GPU ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ ê¸¸ì´ê°€ ê°™ì€ ë²¡í„°ë¥¼ ëª¨ì•„ matrixë¥¼ ë§Œë“¤ê¸° ìœ„í•´ `PAD` í† í°ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤.
ì´ë•Œ, ê·¸ ê¸°ì¤€ì„ ëª‡ìœ¼ë¡œ ì¡ì„ ê²ƒì¸ì§€ ì •í•  í•„ìš”ê°€ ìˆë‹¤.
í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ì™€ í‰ê·  ê¸¸ì´ë¥¼ í™•ì¸í•˜ê³  ì„ê³„ê°’ì„ ì„ì˜ë¡œ ì§€ì •í•´ ê¸¸ì´ê°€ ê·¸ ì´í•˜ì¸ ë¹„ìœ¨ì„ í™•ì¸í•´ë³´ë©° ì ì •í•œ ê°’ì„ ì°¾ì•„ë³¸ë‹¤.

```python
import matplotlib.pyplot as plt

print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´: ', max(len(text) for text, _ in tokenized_train_dataset))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´: ', sum(map(lambda x: len(x[0]), tokenized_train_dataset)) / len(tokenized_train_dataset))

plt.hist([len(text) for text, _ in tokenized_train_dataset], bins=50)
plt.xlabel('length of text')
plt.ylabel('number of text')
plt.show()
```

![](../../assets/HW02/result11.png)

- ìµœëŒ€ ê¸¸ì´ëŠ” `1429` ì´ê³ , í‰ê·  ê¸¸ì´ëŠ” `121` ì´ë‹¤.
- ë˜í•œ ê·¸ë˜í”„ë¥¼ í†µí•´ í™•ì¸í–ˆì„ ë•Œ ëŒ€ëµ 200~350 ë³´ë‹¤ ì‘ì€ í† í°ë“¤ì´ í° ë¹„ì¤‘ì„ ì´ë£¨ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
- ê·¸ëŸ¬ë¯€ë¡œ ìµœëŒ€ ê¸¸ì´ `1429` ë¡œ ëª¨ë“  í…ìŠ¤íŠ¸ì— íŒ¨ë”©ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì€ ì•Œë§ì§€ ì•Šë‹¤.

```python
threshold = 250

count = 0

for text, _ in tokenized_train_dataset:
	if(len(text) <= threshold):
		count += 1

ratio = count / len(tokenized_train_dataset) * 100
print('tokenized_train_dataset ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¨: %s' %(threshold, ratio))
```

- ì„ì˜ì˜ `threshold` ë¥¼ ì„¤ì •í•˜ê³ , í•´ë‹¹ ê¸¸ì´ë¡œ íŒ¨ë”©ì„ í–ˆì„ ë•Œ ëª‡ ê°œì˜ í…ìŠ¤íŠ¸ê°€ ì†ìƒë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•œë‹¤.
- `threshold = 250` ì¸ ê²½ìš° 90%ì˜ í…ìŠ¤íŠ¸ëŠ” ë³´ì¡´í•  ìˆ˜ ìˆë‹¤. 
	- `threshold = 500` : 99%ì˜ í…ìŠ¤íŠ¸ ë³´ì¡´
	- `threshold = 300` : 93%ì˜ í…ìŠ¤íŠ¸ ë³´ì¡´
	- `threshold = 200` : 85%ì˜ í…ìŠ¤íŠ¸ ë³´ì¡´
- ë”°ë¼ì„œ ê¸¸ì´ëŠ” 500ìœ¼ë¡œ ì •í•˜ì˜€ë‹¤.

### 3.2 Dataset, Lightning Module ì •ì˜

```python
import torch  
from torch.utils.data import Dataset, DataLoader  
  
# define dataset class  
class SentimentDataset(Dataset):  
    def __init__(self, data, vocab):  
        self.data = data  
        self.vocab = vocab  
  
    def __len__(self):  
        return len(self.data)  
  
    def __getitem__(self, index):  
        label = int(self.data[index][1])  
        tokens = self.data[index][0]  
  
        token_ids = [self.vocab[token] if token in self.vocab else 1 for token in tokens]  
          
        if len(token_ids) > 500:  
            token_ids = token_ids[:500]  
        else:  
            token_ids = token_ids[:500] + [0] * (500 - len(token_ids))  
  
        return torch.tensor(token_ids), torch.tensor(label)
```

- ìœ„ì—ì„œ ì •í•œ ê²ƒê³¼ ê°™ì´ íŒ¨ë”©ì€ **500** ìœ¼ë¡œ ê²°ì •í•œë‹¤.

```python
import torch.nn as nn  
import lightning as pl  
  
class SentimentClassifierPL(pl.LightningModule):  
    def __init__(self, sentiment_classifier):  
        super(SentimentClassifierPL, self).__init__()  
        self.model = sentiment_classifier  
        self.loss = nn.CrossEntropyLoss()  
          
        self.validation_step_outputs = []  
        self.test_step_outputs = []  
        self.save_hyperparameters()  
      
    def training_step(self, batch, batch_idx):  
        inputs, labels = batch  
        outputs = self.model(inputs)  
        loss = self.loss(outputs, labels)  
        self.log("train_loss", loss)  
        return loss  
      
    def validation_step(self, batch, batch_idx):  
        inputs, labels = batch  
        outputs = self.model(inputs)  
        loss = self.loss(outputs, labels)  
        self.log("val_loss", loss)  
        self.validation_step_outputs.append((loss, outputs, labels))  
        return loss, outputs, labels  
      
    def on_validation_epoch_end(self):  
        outputs = self.validation_step_outputs  
        avg_loss = torch.stack([x[0] for x in outputs]).mean()  
        self.log("avg_val_loss", avg_loss)  
          
        all_outputs = torch.cat([x[1] for x in outputs])  
        all_labels = torch.cat([x[2] for x in outputs])  
        all_preds = all_outputs.argmax(dim=1)  
        accuracy = (all_preds == all_labels).float().mean()  
        self.log("val_accuracy", accuracy)  
        self.validation_step_outputs.clear()  
      
    def test_step(self, batch, batch_idx):  
        inputs, labels = batch  
        outputs = self.model(inputs)  
        loss = self.loss(outputs, labels)  
        self.log("test_loss", loss)  
        self.test_step_outputs.append((loss, outputs, labels))  
        return loss, outputs, labels  
      
    def on_test_epoch_end(self):  
        outputs = self.test_step_outputs  
        avg_loss = torch.stack([x[0] for x in outputs]).mean()  
        self.log("avg_test_loss", avg_loss)  
          
        all_outputs = torch.cat([x[1] for x in outputs])  
        all_labels = torch.cat([x[2] for x in outputs])  
        all_preds = all_outputs.argmax(dim=1)  
        accuracy = (all_preds == all_labels).float().mean()  
        self.log("test_accuracy", accuracy)  
        self.test_step_outputs.clear()  
          
    def configure_optimizers(self):  
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4, weight_decay=0.01)  
        return optimizer
```

- ê¸°ì¡´ ì‹¤ìŠµ ì½”ë“œì—ì„œ **weight_decay = 0.01** ì„ ì¶”ê°€í–ˆë‹¤.
	- `L2 Regularization`

```python
import wandb  
from lightning.pytorch.loggers import WandbLogger  
from lightning.pytorch.callbacks import ModelSummary  
  
wandb.login()  
  
def check_vocab_properties(vocab):  
    print(f"Vocab size: {len(vocab)}")  
    print(f"Vocab items: {list(vocab.items())[:5]}")  
  
  
def check_performance(model, vocab,train_data, test_data, max_epochs, wandb_log_name):  
    wandb_logger = WandbLogger(project="NLP", name=wandb_log_name, group="HW02")  
  
    pl_model = SentimentClassifierPL(model)  
  
    train_dataset = SentimentDataset(train_data, vocab)  
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)  
    val_dataset = SentimentDataset(test_data, vocab)  
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)  
    test_dataset = SentimentDataset(test_data, vocab)  
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)  
  
    trainer = pl.Trainer(  
        max_epochs=max_epochs,  
        accelerator="gpu",  
        logger=wandb_logger,  
        callbacks=[ModelSummary(max_depth=2)]  
    )  
  
    trainer.fit(  
        model=pl_model,  
        train_dataloaders=train_loader,  
        val_dataloaders=val_loader  
    )  
  
    trainer.test(dataloaders=test_loader)  
  
    wandb.finish()
```

- `validation_dataset` ì€ `test_data` ë¡œ ì„¤ì •í•œë‹¤.
- ë˜í•œ, `epoch` ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ì‹¤í—˜í•˜ê¸° ìœ„í•´ `check_performance` í•¨ìˆ˜ì˜ ì¸ìë¡œ ì´ë¥¼ ë°›ë„ë¡ ë³€ê²½í–ˆë‹¤.

### 3.3 ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ê²Œ ë‚˜ì˜¨ ëª¨ë¸ì„ ì œì‹œí•˜ê¸° ì „, ê³¼ì œë¥¼ ì§„í–‰í•˜ë©° ì‹¤í—˜í•œ ê²ƒë“¤ì— ëŒ€í•œ ìš”ì•½ì„ ë¨¼ì € ì œì‹œí•œë‹¤.

![](../../assets/HW02/wandb01.png)

ê°€ì¥ ë¨¼ì € ì‹¤ìŠµ ì‹œê°„ì„ í†µí•´ ë°°ìš´ 4ê°€ì§€ ëª¨ë¸ (MLP, TextCNN, LSTM, BiLSTM) ì„ ì ì ˆíˆ ë³€ê²½í•´ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì•˜ë‹¤.
ì´ê²ƒë“¤ì˜ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- `MLP` : Test Accuracy **84.95%**
- `TextCNN (epoch 2)` : Test Accuracy **87.92%**
- `LSTM` : Test Accuracy **83.13%**
- `TextCNN (epoch 5)` : Test Accuracy **87.43%**
- `BiLSTM` : Test Accuracy **85.84%**

5ê°œì˜ ê²°ê³¼ ì™¸ì—ë„ epoch, vector_size ë“±ì„ ë³€ê²½í•˜ë©° ë§ì€ ì‹œë„ë¥¼ í•´ë´¤ì§€ë§Œ í•´ë‹¹ ê²°ê³¼ë¬¼ë“¤ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒë“¤ì´ì—ˆë‹¤.
ëŒ€ì²´ì ìœ¼ë¡œ RNN ê³„ì—´ì˜ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì´ TextCNNë³´ë‹¤ ë‚®ê²Œ ë‚˜ì™”ë‹¤.
ë”°ë¼ì„œ, TextCNNì„ ì¤‘ì ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í•˜ë ¤ëŠ” ì‹œë„ë¥¼ ì—¬ëŸ¬ì°¨ë¡€ í•´ë³´ì•˜ë‹¤.

![](../../assets/HW02/wandb02.png)

ì—¬ê¸°ì„œ ì‹œë„í•œ ê²ƒë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
- ë¨¼ì €, ê°€ì¥ ìµœì ì˜ epochê°€ ì–´ë””ì¼ ê²ƒì¸ê°€ë¥¼ ì°¾ìœ¼ë ¤ ë…¸ë ¥í–ˆë‹¤.
- ë˜í•œ, SkipGram ëŒ€ì‹  ì‚¬ì „ í•™ìŠµëœ `Glove` ë¥¼ ê°€ì ¸ì™€ ì ìš©í•´ë³´ì•˜ë‹¤.
	- ê²°ê³¼ì ìœ¼ë¡œ **glove50d, glove100d, glove200d** ëª¨ë‘ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ëŠ” ëª»í–ˆë‹¤.
- vector_sizeë¥¼ 300ê¹Œì§€ë„ ëŠ˜ë ¤ë³´ì•˜ì§€ë§Œ, ì¦ê°€í•˜ëŠ” ì„±ëŠ¥ ëŒ€ë¹„ í•™ìŠµ ì†Œìš” ì‹œê°„ì´ ë„ˆë¬´ ê¸¸ì–´ì ¸ ì ì ˆí•œ Trade-offë¥¼ í•´ì•¼ í–ˆë‹¤.
	- ë”°ë¼ì„œ `vector_size` ëŠ” **200** ìœ¼ë¡œ ê²°ì •í•œ ê²ƒì´ ì´ ì‹¤í—˜ìœ¼ë¡œ ì¸í•œ ê²°ê³¼ì´ë‹¤.

### 3.4 ìµœì¢… ëª¨ë¸

![](../../assets/HW02/wandb03.png)

ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•œ ëª¨ë¸ì€ `CNN-LSTM` ëª¨ë¸ì´ë‹¤.

- CNNì´ Sentiment Classification Taskì—ì„œ ê°€ì§€ëŠ” ë‚´ê°€ ìƒê°í•˜ëŠ” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
	- CNNì€ ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œì— íƒì›”í•˜ë¯€ë¡œ ê°ì •ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ì–´ ë° n-gramê³¼ ê°™ì€ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì˜ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.
	
- LSTMì´ Sentiment Classification Taskì—ì„œ ê°€ì§€ëŠ” ë‚´ê°€ ìƒê°í•˜ëŠ” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
	- Sentence, Sequenceì˜ ì „ì²´ êµ¬ì¡° ë° í…ìŠ¤íŠ¸ì˜ ìˆœì„œì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ê²ƒë“¤ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ì¥ì ì´ ìˆë‹¤.
	- ì˜ˆë¥¼ ë“¤ì–´ ê¸ì •ì˜ í‘œí˜„ì´ ë‚˜ì™”ì§€ë§Œ ì „ì²´ë¥¼ ì½ì–´ë³´ë©´ ë¶€ì •ì˜ ë¦¬ë·°ì¸ ê²ƒë“¤ì´ ìˆì„ ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ, ì´ ë‘ ëª¨ë¸ì˜ ì¥ì ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `CNN-LSTM` ëª¨ë¸ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ê³  ì‹¶ì—ˆë‹¤.
- ë¨¼ì € Convolution Layerë¥¼ í†µê³¼í•˜ë©° ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ ë‹¨ì–´ í˜¹ì€ ë¬¸êµ¬ë¥¼ íŒŒì•…í•˜ê³ , ì´ê²ƒì„ LSTMì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•œë‹¤.
- íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ CNNê³¼ Sequence Modelingì„ ìœ„í•œ LSTMì„ í•¨ê»˜ í™œìš©í•´ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ íŒŒì•…í•˜ê³  ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ì§€ ì•Šì„ê¹Œ ê¸°ëŒ€í•œë‹¤.

```python
class CNNwithLSTM(nn.Module):  
    def __init__(self, vocab_size):  
        super(CNNwithLSTM, self).__init__()  
        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_lookup_matrix), freeze=False)  
  
        self.conv1 = nn.Conv2d(1, 128, (3, 200))  
        self.conv2 = nn.Conv2d(1, 128, (5, 200))  
        self.conv3 = nn.Conv2d(1, 128, (7, 200))  
        self.conv4 = nn.Conv2d(1, 128, (9, 200))  
  
        self.dropout = nn.Dropout(0.5)  
        self.bn = nn.BatchNorm1d(128)  
  
        self.rnn = nn.LSTM(128, 128, batch_first=True, num_layers=1, bidirectional=False)  
        self.fc = nn.Linear(4*128, 2)  
      
    def forward(self, x):  
        embedding = self.embedding(x).unsqueeze(1)  
  
        conv1_feature = F.relu(self.conv1(embedding).squeeze(3))  
        conv2_feature = F.relu(self.conv2(embedding).squeeze(3))  
        conv3_feature = F.relu(self.conv3(embedding).squeeze(3))  
        conv4_feature = F.relu(self.conv4(embedding).squeeze(3))  
  
        max1 = F.max_pool1d(conv1_feature, conv1_feature.size(2)).squeeze(2)  
        max2 = F.max_pool1d(conv2_feature, conv2_feature.size(2)).squeeze(2)  
        max3 = F.max_pool1d(conv3_feature, conv3_feature.size(2)).squeeze(2)  
        max4 = F.max_pool1d(conv4_feature, conv4_feature.size(2)).squeeze(2)  
          
        max1 = self.bn(self.dropout(max1))  
        max2 = self.bn(self.dropout(max2))  
        max3 = self.bn(self.dropout(max3))  
        max4 = self.bn(self.dropout(max4))  
  
        rnn1, _ = self.rnn(max1)  
        rnn2, _ = self.rnn(max2)  
        rnn3, _ = self.rnn(max3)  
        rnn4, _ = self.rnn(max4)  
          
        x = torch.cat([rnn1, rnn2, rnn3, rnn4], dim=1)  
  
        x = self.fc(x)  
  
        return x
```

- ëª¨ë¸ì€ Multi Branch í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í–ˆë‹¤.
- 4ê°œì˜ Convolution Layerê°€ ë³‘ë ¬ë¡œ ìœ„ì¹˜í•˜ê³ , ê°ê° ì»¤ë„ì€ `[3, 5, 7, 9]` ë¡œ ê²°ì •í–ˆë‹¤.
	- Conv Layerë¥¼ í†µê³¼í•œ í›„ Activationì€ ReLUë¥¼ ì‚¬ìš©í•œë‹¤.
- ë˜í•œ ê°ê° Max Poolingì„ ì ìš©í•œë‹¤.
- ê·¸ í›„ì— Batch Normalizationê³¼ Dropout (0.5) ë¥¼ ì ìš©í•œë‹¤.
- ê°ê°ì˜ ì¶œë ¥ì€ LSTM ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤.
- LSTM ëª¨ë¸ì˜ ì¶œë ¥ì€ í•˜ë‚˜ë¡œ í•©ì³ì ¸ Fully Conntected Layerë¥¼ í†µê³¼í•´ ìµœì¢… Outputì´ ë‚˜ì˜¨ë‹¤.

```python
cnn_lstm_model = CNNwithLSTM(len(vocab))
check_performance(cnn_lstm_model, vocab, tokenized_train_dataset, tokenized_test_dataset, 3, "cnn_lstm_epoch3_dim200")
```

- ëª¨ë¸ ì„ ì–¸ ë° í•™ìŠµ ì½”ë“œì´ë‹¤.
- ìµœì¢…ì ìœ¼ë¡œ epoch 3ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤.

![](../../assets/HW02/result12.png)

- Model SummaryëŠ” ìœ„ì™€ ê°™ë‹¤.

í•™ìŠµí•œ ìµœì¢… ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![](../../assets/HW02/result13.png)

- **Test Accuracy 88.4%** 

ë§ˆì§€ë§‰ìœ¼ë¡œ Wandb ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![](../../assets/HW02/wandb04.png)


## ğŸ®ğŸŠ ì†Œê°

ì´ë²ˆ ê³¼ì œë¥¼ ì§„í–‰í•˜ë©´ì„œ ì‹¤ìŠµì‹œê°„ì— ë°°ìš´ ëª¨ë¸ë“¤ì„ ê°€ì§€ê³  ì—¬ëŸ¬ê°€ì§€ ë³€í˜•ë„ í•´ë³´ë©´ì„œ ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•´ë³´ì•˜ë‹¤.
ë”¥ëŸ¬ë‹ ê·¸ë¦¬ê³  ìì—°ì–´ì²˜ë¦¬ê°€ ì™œ **ìƒí™©ê³¼ ëª©ì ì— ë§ê²Œ** ì„ íƒí•´ì•¼ í•˜ê³  ì •ë‹µì´ ì—†ëŠ”ì§€ ë‹¤ì‹œê¸ˆ ëŠë‚€ ê²ƒ ê°™ë‹¤.

ì •í™•ë„ 1%ë¥¼ ì˜¬ë¦¬ê¸° ìœ„í•´ ì •ë§ ë§ì´ ê²€ìƒ‰ë„ í•´ë³´ê³  ê³µë¶€í–ˆë˜ ê²ƒë“¤ì„ ë– ì˜¬ë ¤ ë³´ì•˜ëŠ”ë° ìƒê°ë³´ë‹¤ ì •í™•ë„ë¥¼ ë§ì´ ëª» ì˜¬ë¦° ëŠë‚Œì´ ë“ ë‹¤.
ê·¸ëŸ¬í•œ ì´ìœ ëŠ” `TextCNN` ëª¨ë¸ì„ ì‹¤ìŠµ ì½”ë“œì—ì„œ ê°€ì§€ê³ ì™€ì„œ `vector_size = 100` ìœ¼ë¡œ embedding ì„ ë§Œë“¤ê³  í•™ìŠµì„ ëŒë ¸ì„ ë•Œ ë°”ë¡œ 87% ì •ë„ì˜ ì •í™•ë„ê°€ ë‚˜ì™”ê¸° ë•Œë¬¸ì´ë‹¤.
ë³„ ë‹¤ë¥¸ ì‹œë„ë¥¼ ì•ˆí•œ ê²ƒ ê°™ì€ë° ìƒê°ë³´ë‹¤ ê´œì°®ì€ ìˆ˜ì¹˜ê°€ ë‚˜ì™”ë‹¤ëŠ” ê²ƒ, ê·¸ë¦¬ê³  ë‚´ê°€ í•œ ì‹œë„ë“¤ì´ ì •í™•ë„ë¥¼ í¬ê²Œ ë†’ì´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì´ìœ ë¡œ ê·¸ëŸ° ëŠë‚Œì´ ë“œëŠ” ê²ƒì´ ì•„ë‹Œê°€ ìƒê°ì´ ë“ ë‹¤.

ê²°ë¡ ì ìœ¼ë¡œ ì²˜ìŒ ì‹œë„í–ˆë˜ ê²ƒë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ê³  ì„±ëŠ¥ í–¥ìƒì´ ìˆì—ˆë‹¤ëŠ” ê²ƒì— ì˜ì˜ë¥¼ ë‘”ë‹¤.