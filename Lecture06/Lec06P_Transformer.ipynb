{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JksZkj8-zkh"
      },
      "outputs": [],
      "source": [
        "# !pip install lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9YmC2VPJ-zFa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FF2Lg-Zy-zFb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def download_zip(url, output_path):\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"ZIP file downloaded to {output_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
        "\n",
        "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "output_path = \"fra-eng.zip\"\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    download_zip(url, output_path)\n",
        "\n",
        "    path = os.getcwd()\n",
        "    zipfilename = os.path.join(path, output_path)\n",
        "\n",
        "    with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aQ5AbtOc-zFb"
      },
      "outputs": [],
      "source": [
        "def to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zRms_9nN-zFc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ],
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HPsa78yF-zFc"
      },
      "outputs": [],
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input = [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"[SOS] \" + tar_line + \" [EOS]\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "\n",
        "  return encoder_input, decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CXzoHXtY-zFc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인코더의 입력 : [['i', 'went', 'drinking', 'with', 'one', 'of', 'my', 'boyfriend', 's', 'friends', 'and', 'now', 'he', 's', 'furious', 'at', 'me', '.', 'was', 'this', 'friend', 'a', 'guy', 'or', 'a', 'girl', '?', 'a', 'guy', 'obviously', '.', 'why', 'would', 'i', 'go', 'drinking', 'with', 'his', 'female', 'friends', '?', 'yeah', 'you', 're', 'right', '.', 'his', 'name', 'is', 'tom', '.', 'he', 's', 'really', 'hot', 'and', 'i', 'really', 'want', 'to', 'go', 'drinking', 'with', 'him', 'again', '.']]\n",
            "디코더의 입력 : [['[SOS]', 'je', 'suis', 'allee', 'boire', 'avec', 'un', 'ami', 'de', 'mon', 'compagnon', 'et', 'voila', 'qu', 'il', 'est', 'furieux', 'contre', 'moi', '.', 'etait', 'ce', 'un', 'gars', 'ou', 'une', 'fille', '?', 'un', 'gars', 'bien', 'evidemment', '.', 'pourquoi', 'irais', 'je', 'boire', 'avec', 'ses', 'amies', '?', 'ouais', 'ca', 'se', 'comprend', '.', 'il', 's', 'appelle', 'tom', '.', 'il', 'est', 'trop', 'canon', 'et', 'j', 'ai', 'tellement', 'envie', 'd', 'aller', 'prendre', 'un', 'verre', 'avec', 'lui', 'a', 'nouveau', '.', '[EOS]']]\n"
          ]
        }
      ],
      "source": [
        "sents_en_in, sents_fra_in = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[-1:])\n",
        "print('디코더의 입력 :',sents_fra_in[-1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "69a5bC_T-zFc"
      },
      "outputs": [],
      "source": [
        "#split data\n",
        "def split_data(data, train_ratio=0.7, shuffle=True):\n",
        "    data = list(data)\n",
        "    if shuffle:\n",
        "        random.shuffle(data)\n",
        "    n_train = int(len(data) * train_ratio)\n",
        "    train_data = data[:n_train]\n",
        "    test_data = data[n_train:]\n",
        "    return train_data, test_data\n",
        "\n",
        "train_test_ratio = 0.9\n",
        "train, test = split_data(zip(sents_en_in, sents_fra_in), train_test_ratio)\n",
        "train, vali = split_data(train, train_test_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vkcFONPO-zFc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(188515, 20947, 23274)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train), len(vali), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mMQZ8uDU-zFd"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "en_token_cnt = Counter()\n",
        "fr_token_cnt = Counter()\n",
        "\n",
        "for tokens, _ in train:\n",
        "    en_token_cnt.update(tokens)\n",
        "\n",
        "min_count = 2\n",
        "en_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
        "for token, count in en_token_cnt.items():\n",
        "    if count > min_count and token not in en_vocab:\n",
        "        en_vocab[token] = len(en_vocab)\n",
        "\n",
        "\n",
        "for _, tokens in train:\n",
        "    fr_token_cnt.update(tokens)\n",
        "\n",
        "fr_vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[SOS]\": 2, \"[EOS]\": 3}\n",
        "for token, count in fr_token_cnt.items():\n",
        "    if count > min_count and token not in fr_vocab:\n",
        "        fr_vocab[token] = len(fr_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nqAWuyXF-zFd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8295, 11878)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en_vocab), len(fr_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UdlKVrze-zFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JkcHm177-zFd"
      },
      "outputs": [],
      "source": [
        "class EnToFrDataset(Dataset):\n",
        "    def __init__(self, data, en_vocab, fr_vocab):\n",
        "        self.enc_input = []\n",
        "        self.dec_input = []\n",
        "        for en_sent, fr_sent_in in data:\n",
        "            self.enc_input.append(en_sent)\n",
        "            self.dec_input.append(fr_sent_in)\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.max_len = 30\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.enc_input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sample = [self.en_vocab.get(w, self.en_vocab.get(\"[UNK]\")) for w in self.enc_input[idx]]\n",
        "        trg_sample = [self.fr_vocab.get(w, self.fr_vocab.get(\"[UNK]\")) for w in self.dec_input[idx]]\n",
        "        src_sample = src_sample[:self.max_len]\n",
        "        trg_sample = trg_sample[:self.max_len]\n",
        "        src_sample += [self.en_vocab.get(\"[PAD]\")] * (self.max_len - len(src_sample))\n",
        "        trg_sample += [self.fr_vocab.get(\"[PAD]\")] * (self.max_len - len(trg_sample))\n",
        "\n",
        "        return {\"src\": torch.LongTensor(src_sample), \"trg\": torch.LongTensor(trg_sample)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W1hCP6-R-zFd"
      },
      "outputs": [],
      "source": [
        "train_dataset = EnToFrDataset(train, en_vocab, fr_vocab)\n",
        "vali_dataset = EnToFrDataset(vali, en_vocab, fr_vocab)\n",
        "test_dataset = EnToFrDataset(test, en_vocab, fr_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64,drop_last=True, shuffle=True, num_workers=8)\n",
        "vali_loader = DataLoader(vali_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,drop_last=True, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bfpUifxw-zFd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'src': tensor([  20,  187,   33, 4260,   51,   68,   24,    1,   13,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0]),\n",
              " 'trg': tensor([   2,   21,  451,  175,   27, 5705,  143,   27, 3456,   15,    3,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0])}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__getitem__(10120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fql9hTJg-zFd"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # d_model : hidden size의 크기, num_heads : multi-head attention의 head 수\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        # hidden dimension이 num_heads와 나누어 떨어지는지 check!\n",
        "        assert d_model % self.num_heads == 0\n",
        "        # head 별 hidden size 계산\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # projection linear layer\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 최종 output을 위한 dense layer\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    # mask : decoder는 masking 기법을 사용하기 때문에!\n",
        "    # mask : attention에서 PAD 토큰에 대한 masking!\n",
        "    def forward(self, q, k, v, mask):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.Wq(q)\n",
        "        k = self.Wk(k)\n",
        "        v = self.Wv(v)\n",
        "        # [2, 5, 128] : 2개의 batch, 5의 sequence 길이, 128개의 hidden sie\n",
        "        # print(q.size())\n",
        "        # print(k.size())\n",
        "        # print(v.size())\n",
        "        # print('#' * 50)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        # [2, 8, 5, 16] : 2개의 batch, 8개의 head, 5의 sequence 길이, 16개의 hidden size\n",
        "        # print(q.size())\n",
        "        # print(k.size())\n",
        "        # print(v.size())\n",
        "\n",
        "        # attention score\n",
        "        attn = torch.matmul(q, k.permute(0, 1, 3, 2)) / math.sqrt(self.depth)\n",
        "        # mask를 통해 PAD 토큰 연산 X\n",
        "        # mask 되어야 하는 부분은 0, 아닌 것은 1에 -1e9 를 채워넣음!\n",
        "        attn = attn.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
        "        # print(attn[0][0])\n",
        "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
        "        # 과제는 이것을 밖으로 내보내서 Headmap!!!!!!!\n",
        "        \n",
        "        # mask 된 부분은 softmax를 취하면 0이 되길 기대한다!\n",
        "        # print(attn[0][0])\n",
        "        out = torch.matmul(attn, v)\n",
        "        # [2, 4, 6, 32] : 2개의 batch, 4개의 head, 6의 sequence 길이, 32의 hidden size (128//4)\n",
        "        # print(out.size())\n",
        "        \n",
        "        out = out.permute(0, 2, 1, 3).contiguous()\n",
        "        out = out.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.dense(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ygjBDa4k-zFd"
      },
      "outputs": [],
      "source": [
        "class TransformerEncodeLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(TransformerEncodeLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        attn_output = self.mha(x, x, x, padding_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        # x + attn_output : residual connection!\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3JRWtOhh-zFe"
      },
      "outputs": [],
      "source": [
        "class TrasnformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(TrasnformerDecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # self-attention, look_ahead_mask (뒷부분의 정보를 지우는 mask!)\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "        return out3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pkbC-0-J-zFe"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.enc_layers = nn.ModuleList([TransformerEncodeLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, padding_mask)\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.dec_layers = nn.ModuleList([TrasnformerDecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sWsE0sJg-zFe"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate, en_vocab_size, fr_vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.output_dim = fr_vocab_size\n",
        "        self.en_Embedding = nn.Embedding(en_vocab_size, d_model)\n",
        "        self.fr_Embedding = nn.Embedding(fr_vocab_size, d_model)\n",
        "\n",
        "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, dropout_rate)\n",
        "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, dff, dropout_rate)\n",
        "        self.final_layer = nn.Linear(d_model, fr_vocab_size)\n",
        "\n",
        "    def encode(self, enc_input, enc_padding_mask):\n",
        "        return self.encoder(self.en_Embedding(enc_input), enc_padding_mask)\n",
        "\n",
        "    def decode(self, dec_input, enc_output, look_ahead_mask, dec_padding_mask):\n",
        "        return self.decoder(self.fr_Embedding(dec_input), enc_output, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    def forward(self, enc_input, dec_input, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encode(enc_input, enc_padding_mask)\n",
        "        dec_output = self.decode(dec_input, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WbzGwP7i-zFe"
      },
      "outputs": [],
      "source": [
        "def make_pad_mask(query, key, pad_idx=0):\n",
        "    # mask는 batch x head size x sequence length x hidden size 만큼의 mask!\n",
        "    query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "\n",
        "    key_mask = key.ne(pad_idx).unsqueeze(1)\n",
        "    # print(key_mask)\n",
        "    key_mask = key_mask.repeat(1, query_seq_len, 1)\n",
        "    # print(key_mask)\n",
        "\n",
        "    query_mask = query.ne(pad_idx).unsqueeze(2)\n",
        "    # print(query_mask)\n",
        "    query_mask = query_mask.repeat(1,  1, key_seq_len)\n",
        "    # print(query_mask)\n",
        "\n",
        "    # 가로축에 대한 mask (query mask), 세로축에 대한 mask (key mask) 를 합치는 연산!\n",
        "    mask = key_mask & query_mask\n",
        "    # print(mask)\n",
        "    mask.requires_grad = False\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ai5Va74J-zFe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 11878])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_layers = 2\n",
        "d_model = 128\n",
        "num_heads = 1\n",
        "dff = 512\n",
        "dropout_rate = 0.1\n",
        "en_vocab_size = len(en_vocab)\n",
        "fr_vocab_size = len(fr_vocab)\n",
        "\n",
        "model = Transformer(num_layers, d_model, num_heads, dff, dropout_rate, en_vocab_size, fr_vocab_size)\n",
        "\n",
        "enc_input = torch.LongTensor([[1, 2, 3, 0, 0], [6, 7, 8, 9, 10]])\n",
        "dec_input = torch.LongTensor([[1, 2, 3, 4, 5, 6], [6, 7, 8, 9, 10,11]])\n",
        "\n",
        "enc_padding_mask = make_pad_mask(enc_input, enc_input)\n",
        "dec_padding_mask = make_pad_mask(dec_input, enc_input)\n",
        "\n",
        "# decoder에 들어가는 미래 정보를 지워주는 mask!\n",
        "# triu : upper triangle을 만드는 pytorch 연산!\n",
        "# causal_mask : 자주 사용하는 mask! 이러한 마스크를 이용하는 것을 causal LM\n",
        "look_ahead_mask = ~torch.triu(torch.ones(dec_input.size(1), dec_input.size(1), dtype=torch.bool), diagonal=1).unsqueeze(0)\n",
        "# print(look_ahead_mask)\n",
        "\n",
        "model(enc_input, dec_input, enc_padding_mask, look_ahead_mask, dec_padding_mask).size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I2xNg1_V-zFe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "import lightning as pl\n",
        "\n",
        "class TransformerPL(pl.LightningModule):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterian = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def make_pad_mask(self, query, key, pad_idx=0):\n",
        "        query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "\n",
        "        key_mask = key.ne(pad_idx).unsqueeze(1)\n",
        "        key_mask = key_mask.repeat(1, query_seq_len, 1)\n",
        "\n",
        "        query_mask = query.ne(pad_idx).unsqueeze(2)\n",
        "        query_mask = query_mask.repeat(1,  1, key_seq_len)\n",
        "\n",
        "        mask = key_mask & query_mask\n",
        "        mask.requires_grad = False\n",
        "        return mask.to(query.device)\n",
        "\n",
        "    def make_causal_mask(self, query, pad_idx=0):\n",
        "        seq_len = query.size(1)\n",
        "        causal_mask = ~torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0)\n",
        "        return causal_mask.to(query.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        enc_padding_mask = self.make_pad_mask(src, src)\n",
        "        dec_padding_mask = self.make_pad_mask(trg, src)\n",
        "        look_ahead_mask = self.make_causal_mask(trg)\n",
        "\n",
        "        outputs = self.model(src, trg, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src = batch[\"src\"]\n",
        "        trg = batch[\"trg\"]\n",
        "\n",
        "        outputs = self(src, trg)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        # loss 계산의 편의를 위해 펼쳐주는 연산\n",
        "        # outputs[:, :-1] : 한개씩 밀린 것과 비교, 자기 자신과 비교 X\n",
        "        outputs = outputs[:,:-1].reshape(-1, outputs_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_PPL\", math.exp(loss))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src = batch[\"src\"]\n",
        "        trg = batch[\"trg\"]\n",
        "\n",
        "        outputs = self(src, trg)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        outputs = outputs[:,:-1].reshape(-1, outputs_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_PPL\", math.exp(loss))\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        src = batch[\"src\"]\n",
        "        trg = batch[\"trg\"]\n",
        "\n",
        "        outputs = self(src, trg)\n",
        "\n",
        "        outputs_dim = outputs.shape[-1]\n",
        "        outputs = outputs[:,:-1].reshape(-1, outputs_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = self.criterian(outputs, trg)\n",
        "\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_PPL\", math.exp(loss))\n",
        "        return loss\n",
        "\n",
        "    def decode(self, src):\n",
        "        enc_output = self.model.encode(src, self.make_pad_mask(src, src))\n",
        "        trg_len = 30\n",
        "        outputs = [2]\n",
        "        input = torch.LongTensor([[2] for _ in range(src.size(0))]).to(src.device)\n",
        "        for t in range(1, trg_len):\n",
        "            look_ahead_mask = self.make_causal_mask(input)\n",
        "            dec_padding_mask = self.make_pad_mask(input, src)\n",
        "            output = self.model.decode(input, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "            output = self.model.final_layer(output)\n",
        "            output = output[:,-1,:]\n",
        "            top1 = output.argmax(1)\n",
        "            outputs.append(top1.item())\n",
        "            if top1.item() == 3:\n",
        "                break\n",
        "            input = torch.cat([input, top1.unsqueeze(1)], dim=1)\n",
        "        return outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NcdhtWHb-zFe"
      },
      "outputs": [],
      "source": [
        "num_layers = 2\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "dropout_rate = 0.1\n",
        "en_vocab_size = len(en_vocab)\n",
        "fr_vocab_size = len(fr_vocab)\n",
        "\n",
        "transformer_model = Transformer(num_layers, d_model, num_heads, dff, dropout_rate, en_vocab_size, fr_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_sYRbj0N-zFe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
          ]
        }
      ],
      "source": [
        "model_pl = TransformerPL(transformer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hYKALkL7-zFe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dev/anaconda3/envs/nlp/lib/python3.12/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(9.5751, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trainingstep test\n",
        "batch = next(iter(train_loader))\n",
        "model_pl.training_step(batch, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4pOzQVWS-zFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoeyhesx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb_logger = WandbLogger(project=\"NLP\", name=\"Transformer\", group=\"Lec06\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "skJ1fmqf-zFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=1,\n",
        "    accelerator=\"gpu\",\n",
        "    logger=wandb_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T1HAuMMx-zFf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "wandb version 0.16.6 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20240501_154220-fjxm28vh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/noeyhesx/NLP/runs/fjxm28vh/workspace' target=\"_blank\">Transformer</a></strong> to <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/noeyhesx/NLP' target=\"_blank\">https://wandb.ai/noeyhesx/NLP</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/noeyhesx/NLP/runs/fjxm28vh/workspace' target=\"_blank\">https://wandb.ai/noeyhesx/NLP/runs/fjxm28vh/workspace</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | Transformer      | 5.0 M \n",
            "1 | criterian | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "5.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.0 M     Total params\n",
            "20.160    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [00:43<00:00, 67.58it/s, v_num=28vh, train_loss=3.160]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2945/2945 [00:43<00:00, 67.43it/s, v_num=28vh, train_loss=3.160]\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model_pl, train_loader, vali_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v2bHO-4n-zFf"
      },
      "outputs": [],
      "source": [
        "test_data = test_dataset.__getitem__(1122)\n",
        "generate_output = model_pl.decode(test_data[\"src\"].unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-nYto8ca-zFf"
      },
      "outputs": [],
      "source": [
        "input = \" \".join([list(en_vocab.keys())[list(en_vocab.values()).index(i)] for i in test_data[\"src\"]])\n",
        "target = \" \".join([list(fr_vocab.keys())[list(fr_vocab.values()).index(i)] for i in test_data[\"trg\"]])\n",
        "model_output = \" \".join([list(fr_vocab.keys())[list(fr_vocab.values()).index(i)] for i in generate_output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "PH5qeeRP-zFf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we re sorry for what happened to you . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[SOS] nous sommes desoles pour ce qui vous est arrive . [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[SOS] nous avons ce que nous sommes a ce que tu es pour ce que vous avez . [EOS]\n"
          ]
        }
      ],
      "source": [
        "print(input)\n",
        "print(target)\n",
        "print(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HV4TyN3g-zFf"
      },
      "outputs": [],
      "source": [
        "trainer.save_checkpoint(\"transformer.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_6HEh3I-zFf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
